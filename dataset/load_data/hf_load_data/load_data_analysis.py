#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Load Data Analysis Tool
Comprehensive analysis of load_data dataset for time series forecasting
Created on: 2024-09-21
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import json
import os
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Statistical analysis
from scipy import stats

# Try to import optional packages with fallbacks
try:
    import seaborn as sns
    HAS_SEABORN = True
except ImportError:
    HAS_SEABORN = False
    print("âš ï¸  Seaborn not available - some visualizations will be simplified")

try:
    from statsmodels.tsa.stattools import adfuller
    from statsmodels.tsa.seasonal import seasonal_decompose
    from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
    HAS_STATSMODELS = True
except ImportError:
    HAS_STATSMODELS = False
    print("âš ï¸  Statsmodels not available - some time series analyses will be skipped")

try:
    from sklearn.preprocessing import StandardScaler
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    print("âš ï¸  Scikit-learn not available - some analyses will be simplified")

# Set plotting style
plt.style.use('default')
if HAS_SEABORN:
    sns.set_palette("husl")

class LoadDataAnalyzer:
    def __init__(self, data_path=None):
        """
        Initialize the Load Data Analyzer
        
        Args:
            data_path (str): Path to the CSV file, if None uses default path
        """
        if data_path is None:
            # Use current directory as default
            self.data_path = "./hf_load_data_20210101-20250807_processed.csv"
        else:
            self.data_path = data_path
            
        # Create results directory in current folder
        self.results_dir = Path("./analysis_results")
        self.plots_dir = self.results_dir / "plots"
        self.reports_dir = self.results_dir / "reports"
        
        # Create directories
        for dir_path in [self.results_dir, self.plots_dir, self.reports_dir]:
            dir_path.mkdir(exist_ok=True)
            
        self.df = None
        self.analysis_results = {}
        
    def load_data(self):
        """Load and initial check of the dataset"""
        print("ğŸ” Loading load_data dataset...")
        
        try:
            self.df = pd.read_csv(self.data_path)
            print(f"âœ… Data loaded successfully: {self.df.shape}")
            
            # Display basic info
            print(f"ğŸ“Š Dataset shape: {self.df.shape}")
            print(f"ğŸ“‹ Columns: {list(self.df.columns)}")
            print(f"ğŸ“… Date range: {self.df.iloc[0, 0]} to {self.df.iloc[-1, 0]}")
            
            # Convert date column to datetime if exists
            if 'date' in self.df.columns:
                self.df['date'] = pd.to_datetime(self.df['date'])
                self.df.set_index('date', inplace=True)
            elif self.df.columns[0].lower() in ['date', 'time', 'timestamp']:
                self.df.iloc[:, 0] = pd.to_datetime(self.df.iloc[:, 0])
                self.df.set_index(self.df.columns[0], inplace=True)
                
            return True
            
        except Exception as e:
            print(f"âŒ Error loading data: {e}")
            return False
    
    def basic_statistics(self):
        """Perform basic statistical analysis"""
        print("\nğŸ“ˆ Performing basic statistical analysis...")
        
        results = {}
        
        # Basic info
        results['data_shape'] = self.df.shape
        results['columns'] = list(self.df.columns)
        results['data_types'] = self.df.dtypes.to_dict()
        
        # Missing values
        missing_info = self.df.isnull().sum().to_dict()
        results['missing_values'] = missing_info
        results['missing_percentage'] = {col: (count/len(self.df))*100 for col, count in missing_info.items()}
        
        # Descriptive statistics for numerical columns
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        results['descriptive_stats'] = {}
        
        for col in numeric_cols:
            col_stats = {
                'count': int(self.df[col].count()),
                'mean': float(self.df[col].mean()),
                'std': float(self.df[col].std()),
                'min': float(self.df[col].min()),
                'q25': float(self.df[col].quantile(0.25)),
                'median': float(self.df[col].median()),
                'q75': float(self.df[col].quantile(0.75)),
                'max': float(self.df[col].max()),
                'skewness': float(stats.skew(self.df[col].dropna())),
                'kurtosis': float(stats.kurtosis(self.df[col].dropna())),
                'range': float(self.df[col].max() - self.df[col].min()),
                'iqr': float(self.df[col].quantile(0.75) - self.df[col].quantile(0.25))
            }
            results['descriptive_stats'][col] = col_stats
        
        # Save results
        with open(self.reports_dir / 'basic_statistics.json', 'w') as f:
            json.dump(results, f, indent=4, default=str)
            
        self.analysis_results['basic_statistics'] = results
        print("âœ… Basic statistics analysis completed!")
        
        return results
    
    def time_series_analysis(self):
        """Analyze time series properties"""
        print("\nâ° Performing time series analysis...")
        
        results = {}
        
        # Assume the main value column is 'value' or the last numeric column
        value_col = 'value' if 'value' in self.df.columns else self.df.select_dtypes(include=[np.number]).columns[-1]
        ts_data = self.df[value_col].dropna()
        
        # Frequency analysis
        if isinstance(self.df.index, pd.DatetimeIndex):
            results['frequency'] = str(pd.infer_freq(self.df.index))
            results['date_range'] = {
                'start': str(self.df.index.min()),
                'end': str(self.df.index.max()),
                'duration_days': (self.df.index.max() - self.df.index.min()).days
            }
        
        # Trend analysis (linear regression)
        x = np.arange(len(ts_data))
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, ts_data)
        results['trend_analysis'] = {
            'slope': float(slope),
            'intercept': float(intercept),
            'r_squared': float(r_value**2),
            'p_value': float(p_value),
            'trend_direction': 'increasing' if slope > 0 else 'decreasing'
        }
        
        # Stationarity test (ADF test)
        if HAS_STATSMODELS:
            try:
                adf_result = adfuller(ts_data)
                results['stationarity_adf'] = {
                    'adf_statistic': float(adf_result[0]),
                    'p_value': float(adf_result[1]),
                    'critical_values': {k: float(v) for k, v in adf_result[4].items()},
                    'is_stationary': adf_result[1] < 0.05
                }
            except Exception as e:
                results['stationarity_adf'] = {'error': str(e)}
        else:
            results['stationarity_adf'] = {'note': 'Statsmodels not available for ADF test'}
        
        # Autocorrelation analysis
        autocorr_lags = min(50, len(ts_data)//4)
        autocorr = [ts_data.autocorr(lag=i) for i in range(1, autocorr_lags+1)]
        results['autocorrelation'] = {
            'lags_1_to_10': [float(x) for x in autocorr[:10]],
            'significant_autocorr_count': sum(1 for x in autocorr if abs(x) > 0.1)
        }
        
        # Seasonality detection
        if len(ts_data) > 24:  # Need sufficient data for seasonality
            try:
                # Test for different seasonal periods
                seasonal_periods = [24, 168, 8760]  # Daily, weekly, yearly (for hourly data)
                results['seasonality'] = {}
                
                for period in seasonal_periods:
                    if len(ts_data) > 2 * period:
                        # Simple seasonal correlation test
                        seasonal_corr = np.corrcoef(ts_data[:-period], ts_data[period:])[0, 1]
                        results['seasonality'][f'period_{period}'] = float(seasonal_corr)
                        
            except Exception as e:
                results['seasonality'] = {'error': str(e)}
        
        # Save results
        with open(self.reports_dir / 'time_series_analysis.json', 'w') as f:
            json.dump(results, f, indent=4, default=str)
            
        self.analysis_results['time_series_analysis'] = results
        print("âœ… Time series analysis completed!")
        
        return results
    
    def data_quality_assessment(self):
        """Assess data quality"""
        print("\nğŸ” Performing data quality assessment...")
        
        results = {}
        
        # Missing value patterns
        results['missing_patterns'] = self.df.isnull().sum().to_dict()
        
        # Data consistency checks
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        results['consistency_checks'] = {}
        
        for col in numeric_cols:
            col_data = self.df[col].dropna()
            results['consistency_checks'][col] = {
                'negative_values_count': int((col_data < 0).sum()),
                'zero_values_count': int((col_data == 0).sum()),
                'infinite_values_count': int(np.isinf(col_data).sum()),
                'outliers_iqr_count': self._count_outliers_iqr(col_data),
                'outliers_zscore_count': self._count_outliers_zscore(col_data)
            }
        
        # Date gaps (if datetime index)
        if isinstance(self.df.index, pd.DatetimeIndex):
            expected_freq = pd.infer_freq(self.df.index)
            if expected_freq:
                full_range = pd.date_range(start=self.df.index.min(), 
                                         end=self.df.index.max(), 
                                         freq=expected_freq)
                missing_dates = len(full_range) - len(self.df.index)
                results['date_gaps'] = {
                    'expected_records': len(full_range),
                    'actual_records': len(self.df.index),
                    'missing_records': missing_dates,
                    'completeness_percentage': (len(self.df.index) / len(full_range)) * 100
                }
        
        # Overall quality score
        quality_score = self._calculate_quality_score()
        results['overall_quality_score'] = quality_score
        
        # Save results
        with open(self.reports_dir / 'data_quality_assessment.json', 'w') as f:
            json.dump(results, f, indent=4, default=str)
            
        self.analysis_results['data_quality_assessment'] = results
        print("âœ… Data quality assessment completed!")
        
        return results
    
    def _count_outliers_iqr(self, data):
        """Count outliers using IQR method"""
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        return int(((data < lower_bound) | (data > upper_bound)).sum())
    
    def _count_outliers_zscore(self, data, threshold=3):
        """Count outliers using Z-score method"""
        z_scores = np.abs(stats.zscore(data))
        return int((z_scores > threshold).sum())
    
    def _calculate_quality_score(self):
        """Calculate overall data quality score (0-100)"""
        score = 100
        
        # Penalize for missing values
        missing_ratio = self.df.isnull().sum().sum() / (self.df.shape[0] * self.df.shape[1])
        score -= missing_ratio * 30
        
        # Penalize for inconsistencies
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            col_data = self.df[col].dropna()
            if len(col_data) > 0:
                outlier_ratio = self._count_outliers_iqr(col_data) / len(col_data)
                score -= outlier_ratio * 10
        
        return max(0, score)
    
    def pattern_analysis(self):
        """Analyze patterns in the data"""
        print("\nğŸ”„ Performing pattern analysis...")
        
        results = {}
        
        # Main value column
        value_col = 'value' if 'value' in self.df.columns else self.df.select_dtypes(include=[np.number]).columns[-1]
        ts_data = self.df[value_col].dropna()
        
        # Periodic patterns
        if isinstance(self.df.index, pd.DatetimeIndex):
            df_with_time = self.df.copy()
            df_with_time['hour'] = df_with_time.index.hour
            df_with_time['day_of_week'] = df_with_time.index.dayofweek
            df_with_time['month'] = df_with_time.index.month
            
            results['periodic_patterns'] = {}
            
            # Hourly patterns
            if 'hour' in df_with_time.columns:
                hourly_stats = df_with_time.groupby('hour')[value_col].agg(['mean', 'std']).to_dict()
                results['periodic_patterns']['hourly'] = {
                    'mean_by_hour': {str(k): float(v) for k, v in hourly_stats['mean'].items()},
                    'std_by_hour': {str(k): float(v) for k, v in hourly_stats['std'].items()},
                    'peak_hour': int(df_with_time.groupby('hour')[value_col].mean().idxmax()),
                    'low_hour': int(df_with_time.groupby('hour')[value_col].mean().idxmin())
                }
            
            # Weekly patterns
            weekly_stats = df_with_time.groupby('day_of_week')[value_col].agg(['mean', 'std']).to_dict()
            results['periodic_patterns']['weekly'] = {
                'mean_by_day': {str(k): float(v) for k, v in weekly_stats['mean'].items()},
                'std_by_day': {str(k): float(v) for k, v in weekly_stats['std'].items()},
                'peak_day': int(df_with_time.groupby('day_of_week')[value_col].mean().idxmax()),
                'low_day': int(df_with_time.groupby('day_of_week')[value_col].mean().idxmin())
            }
            
            # Monthly patterns
            monthly_stats = df_with_time.groupby('month')[value_col].agg(['mean', 'std']).to_dict()
            results['periodic_patterns']['monthly'] = {
                'mean_by_month': {str(k): float(v) for k, v in monthly_stats['mean'].items()},
                'std_by_month': {str(k): float(v) for k, v in monthly_stats['std'].items()},
                'peak_month': int(df_with_time.groupby('month')[value_col].mean().idxmax()),
                'low_month': int(df_with_time.groupby('month')[value_col].mean().idxmin())
            }
        
        # Trend changes (change point detection - simple)
        window_size = min(100, len(ts_data)//10)
        if window_size > 10:
            rolling_mean = ts_data.rolling(window=window_size).mean()
            rolling_std = ts_data.rolling(window=window_size).std()
            
            results['trend_changes'] = {
                'high_volatility_periods': int((rolling_std > rolling_std.quantile(0.9)).sum()),
                'trend_reversals': self._detect_trend_reversals(rolling_mean),
                'volatility_range': {
                    'min_std': float(rolling_std.min()),
                    'max_std': float(rolling_std.max()),
                    'mean_std': float(rolling_std.mean())
                }
            }
        
        # Extreme values analysis
        results['extreme_values'] = {
            'top_5_values': ts_data.nlargest(5).tolist(),
            'bottom_5_values': ts_data.nsmallest(5).tolist(),
            'extreme_dates': {
                'max_date': str(ts_data.idxmax()),
                'min_date': str(ts_data.idxmin())
            }
        }
        
        # Save results
        with open(self.reports_dir / 'pattern_analysis.json', 'w') as f:
            json.dump(results, f, indent=4, default=str)
            
        self.analysis_results['pattern_analysis'] = results
        print("âœ… Pattern analysis completed!")
        
        return results
    
    def _detect_trend_reversals(self, data):
        """Detect trend reversals in the data"""
        if len(data) < 3:
            return 0
            
        diffs = np.diff(data.dropna())
        if len(diffs) < 2:
            return 0
            
        # Count sign changes in the differences
        sign_changes = np.sum(np.diff(np.sign(diffs)) != 0)
        return int(sign_changes)
    
    def cyclical_trend_analysis(self):
        """æ·±åº¦åˆ†æçŸ­å‘¨æœŸå’Œé•¿å‘¨æœŸçš„è¶‹åŠ¿å˜åŒ–"""
        print("\nğŸ”„ Performing cyclical and trend analysis...")
        
        results = {}
        value_col = 'value' if 'value' in self.df.columns else self.df.select_dtypes(include=[np.number]).columns[-1]
        
        if not isinstance(self.df.index, pd.DatetimeIndex):
            print("âš ï¸  éœ€è¦æ—¶é—´ç´¢å¼•è¿›è¡Œå‘¨æœŸæ€§åˆ†æ")
            return results
        
        # çŸ­å‘¨æœŸåˆ†æ (æ—¥ã€å‘¨)
        short_cycle_results = self._analyze_short_cycles(value_col)
        results['short_cycles'] = short_cycle_results
        
        # é•¿å‘¨æœŸåˆ†æ (æœˆã€å­£ã€å¹´)
        long_cycle_results = self._analyze_long_cycles(value_col)
        results['long_cycles'] = long_cycle_results
        
        # å…¸å‹æ¡ˆä¾‹åˆ†æ
        case_study_results = self._generate_case_studies(value_col)
        results['case_studies'] = case_study_results
        
        # Save results
        with open(self.reports_dir / 'cyclical_trend_analysis.json', 'w') as f:
            json.dump(results, f, indent=4, default=str)
            
        self.analysis_results['cyclical_trend_analysis'] = results
        print("âœ… Cyclical and trend analysis completed!")
        
        return results
    
    def _analyze_short_cycles(self, value_col):
        """åˆ†æçŸ­å‘¨æœŸæ¨¡å¼ (å°æ—¶ã€æ—¥ã€å‘¨)"""
        results = {}
        
        # æ¯æ—¥æ¨¡å¼åˆ†æ
        daily_pattern = self._analyze_daily_pattern(value_col)
        results['daily_pattern'] = daily_pattern
        
        # æ¯å‘¨æ¨¡å¼åˆ†æ
        weekly_pattern = self._analyze_weekly_pattern(value_col)
        results['weekly_pattern'] = weekly_pattern
        
        # å·¥ä½œæ—¥vså‘¨æœ«å¯¹æ¯”
        weekday_weekend = self._analyze_weekday_weekend(value_col)
        results['weekday_weekend'] = weekday_weekend
        
        return results
    
    def _analyze_long_cycles(self, value_col):
        """åˆ†æé•¿å‘¨æœŸæ¨¡å¼ (æœˆã€å­£ã€å¹´)"""
        results = {}
        
        # æœˆåº¦æ¨¡å¼åˆ†æ
        monthly_pattern = self._analyze_monthly_pattern(value_col)
        results['monthly_pattern'] = monthly_pattern
        
        # å­£èŠ‚æ€§æ¨¡å¼åˆ†æ
        seasonal_pattern = self._analyze_seasonal_pattern(value_col)
        results['seasonal_pattern'] = seasonal_pattern
        
        # å¹´åº¦è¶‹åŠ¿åˆ†æ
        yearly_trend = self._analyze_yearly_trend(value_col)
        results['yearly_trend'] = yearly_trend
        
        return results
    
    def _analyze_daily_pattern(self, value_col):
        """åˆ†ææ—¥å†…è´Ÿè·å˜åŒ–æ¨¡å¼"""
        df_hourly = self.df.copy()
        df_hourly['hour'] = df_hourly.index.hour
        
        hourly_stats = df_hourly.groupby('hour')[value_col].agg(['mean', 'std', 'min', 'max']).round(2)
        
        # è®¡ç®—å³°è°·å·®å’Œå˜åŒ–ç‡
        daily_min = hourly_stats['mean'].min()
        daily_max = hourly_stats['mean'].max()
        peak_valley_ratio = (daily_max - daily_min) / daily_min * 100
        
        # è¯†åˆ«è´Ÿè·ç‰¹å¾æ—¶æ®µ
        peak_hour = hourly_stats['mean'].idxmax()
        valley_hour = hourly_stats['mean'].idxmin()
        
        return {
            'hourly_statistics': hourly_stats.to_dict(),
            'peak_hour': int(peak_hour),
            'valley_hour': int(valley_hour),
            'daily_min_load': float(daily_min),
            'daily_max_load': float(daily_max),
            'peak_valley_ratio': float(peak_valley_ratio),
            'load_factor': float(hourly_stats['mean'].mean() / daily_max * 100)
        }
    
    def _analyze_weekly_pattern(self, value_col):
        """åˆ†æå‘¨å†…è´Ÿè·å˜åŒ–æ¨¡å¼"""
        df_weekly = self.df.copy()
        df_weekly['day_of_week'] = df_weekly.index.dayofweek
        df_weekly['day_name'] = df_weekly.index.day_name()
        
        weekly_stats = df_weekly.groupby(['day_of_week', 'day_name'])[value_col].agg(['mean', 'std']).round(2)
        
        # è®¡ç®—å·¥ä½œæ—¥å’Œå‘¨æœ«çš„å¹³å‡è´Ÿè·
        workday_load = df_weekly[df_weekly['day_of_week'] < 5][value_col].mean()
        weekend_load = df_weekly[df_weekly['day_of_week'] >= 5][value_col].mean()
        
        # è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼
        weekly_stats_dict = {}
        for (day_num, day_name), row in weekly_stats.iterrows():
            key = f"{day_num}_{day_name}"
            weekly_stats_dict[key] = {
                'mean': float(row['mean']),
                'std': float(row['std'])
            }
        
        peak_idx = weekly_stats['mean'].idxmax()
        low_idx = weekly_stats['mean'].idxmin()
        
        return {
            'weekly_statistics': weekly_stats_dict,
            'workday_average': float(workday_load),
            'weekend_average': float(weekend_load),
            'workday_weekend_ratio': float(workday_load / weekend_load),
            'peak_day': peak_idx[1] if isinstance(peak_idx, tuple) else str(peak_idx),
            'low_day': low_idx[1] if isinstance(low_idx, tuple) else str(low_idx)
        }
    
    def _analyze_weekday_weekend(self, value_col):
        """åˆ†æå·¥ä½œæ—¥ä¸å‘¨æœ«çš„è´Ÿè·å·®å¼‚"""
        df_compare = self.df.copy()
        df_compare['is_weekend'] = df_compare.index.dayofweek >= 5
        df_compare['hour'] = df_compare.index.hour
        
        # æŒ‰å°æ—¶æ¯”è¾ƒå·¥ä½œæ—¥å’Œå‘¨æœ«
        comparison = df_compare.groupby(['hour', 'is_weekend'])[value_col].mean().unstack()
        
        # è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼
        comparison_dict = {}
        if not comparison.empty:
            for hour in comparison.index:
                hour_data = {}
                if False in comparison.columns:
                    hour_data['weekday'] = float(comparison.loc[hour, False])
                if True in comparison.columns:
                    hour_data['weekend'] = float(comparison.loc[hour, True])
                comparison_dict[str(hour)] = hour_data
        
        # è®¡ç®—å·®å¼‚ç»Ÿè®¡
        if False in comparison.columns and True in comparison.columns:
            hourly_diff = comparison[False] - comparison[True]  # å·¥ä½œæ—¥ - å‘¨æœ«
            max_diff_hour = hourly_diff.idxmax()
            max_diff_value = hourly_diff.max()
            avg_diff = hourly_diff.mean()
        else:
            max_diff_hour = 0
            max_diff_value = 0
            avg_diff = 0
        
        return {
            'hourly_comparison': comparison_dict,
            'max_difference_hour': int(max_diff_hour),
            'max_difference_value': float(max_diff_value),
            'average_difference': float(avg_diff)
        }
    
    def _analyze_monthly_pattern(self, value_col):
        """åˆ†ææœˆåº¦è´Ÿè·å˜åŒ–æ¨¡å¼"""
        df_monthly = self.df.copy()
        df_monthly['month'] = df_monthly.index.month
        df_monthly['month_name'] = df_monthly.index.month_name()
        
        monthly_stats = df_monthly.groupby(['month', 'month_name'])[value_col].agg(['mean', 'std', 'min', 'max']).round(2)
        
        # è®¡ç®—å­£èŠ‚æ€§æŒ‡æ ‡
        spring_load = df_monthly[df_monthly['month'].isin([3, 4, 5])][value_col].mean()
        summer_load = df_monthly[df_monthly['month'].isin([6, 7, 8])][value_col].mean()
        autumn_load = df_monthly[df_monthly['month'].isin([9, 10, 11])][value_col].mean()
        winter_load = df_monthly[df_monthly['month'].isin([12, 1, 2])][value_col].mean()
        
        # è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼
        monthly_stats_dict = {}
        for (month_num, month_name), row in monthly_stats.iterrows():
            key = f"{month_num}_{month_name}"
            monthly_stats_dict[key] = {
                'mean': float(row['mean']),
                'std': float(row['std']),
                'min': float(row['min']),
                'max': float(row['max'])
            }
        
        peak_idx = monthly_stats['mean'].idxmax()
        low_idx = monthly_stats['mean'].idxmin()
        
        return {
            'monthly_statistics': monthly_stats_dict,
            'seasonal_averages': {
                'spring': float(spring_load),
                'summer': float(summer_load),
                'autumn': float(autumn_load),
                'winter': float(winter_load)
            },
            'peak_month': peak_idx[1] if isinstance(peak_idx, tuple) else str(peak_idx),
            'low_month': low_idx[1] if isinstance(low_idx, tuple) else str(low_idx)
        }
    
    def _analyze_seasonal_pattern(self, value_col):
        """åˆ†æå­£èŠ‚æ€§è´Ÿè·ç‰¹å¾"""
        df_seasonal = self.df.copy()
        df_seasonal['month'] = df_seasonal.index.month
        df_seasonal['season'] = pd.Series(df_seasonal['month']).apply(self._get_season).values
        
        seasonal_stats = df_seasonal.groupby('season')[value_col].agg(['mean', 'std', 'min', 'max']).round(2)
        
        # è®¡ç®—å­£èŠ‚æ€§å˜åŒ–ç³»æ•°
        seasonal_cv = (seasonal_stats['std'] / seasonal_stats['mean'] * 100).round(2)
        
        return {
            'seasonal_statistics': seasonal_stats.to_dict(),
            'seasonal_cv': seasonal_cv.to_dict(),
            'highest_season': seasonal_stats['mean'].idxmax(),
            'lowest_season': seasonal_stats['mean'].idxmin()
        }
    
    def _analyze_yearly_trend(self, value_col):
        """åˆ†æå¹´åº¦è¶‹åŠ¿å˜åŒ–"""
        df_yearly = self.df.copy()
        df_yearly['year'] = df_yearly.index.year
        
        yearly_stats = df_yearly.groupby('year')[value_col].agg(['mean', 'std', 'min', 'max']).round(2)
        
        # è®¡ç®—å¹´åº¦å¢é•¿ç‡
        if len(yearly_stats) > 1:
            growth_rates = yearly_stats['mean'].pct_change().dropna() * 100
            avg_growth_rate = growth_rates.mean()
        else:
            growth_rates = pd.Series()
            avg_growth_rate = 0
        
        return {
            'yearly_statistics': yearly_stats.to_dict(),
            'growth_rates': growth_rates.to_dict(),
            'average_growth_rate': float(avg_growth_rate),
            'trend_direction': 'increasing' if avg_growth_rate > 0 else 'decreasing'
        }
    
    def _get_season(self, month):
        """æ ¹æ®æœˆä»½åˆ¤æ–­å­£èŠ‚"""
        if month in [3, 4, 5]:
            return 'Spring'
        elif month in [6, 7, 8]:
            return 'Summer'
        elif month in [9, 10, 11]:
            return 'Autumn'
        else:
            return 'Winter'
    
    def _generate_case_studies(self, value_col):
        """ç”Ÿæˆå…¸å‹æ¡ˆä¾‹åˆ†æ"""
        results = {}
        
        # é€‰æ‹©ä»£è¡¨æ€§çš„æ—¶é—´æ®µè¿›è¡Œæ¡ˆä¾‹åˆ†æ
        case_periods = {
            'typical_day': self._get_typical_day(value_col),
            'peak_week': self._get_peak_week(value_col),
            'seasonal_comparison': self._get_seasonal_comparison(value_col)
        }
        
        for case_name, case_data in case_periods.items():
            if case_data is not None:
                results[case_name] = case_data
        
        return results
    
    def _get_typical_day(self, value_col):
        """è·å–å…¸å‹æ—¥è´Ÿè·æ›²çº¿"""
        # é€‰æ‹©ä¸€ä¸ªå·¥ä½œæ—¥ä½œä¸ºå…¸å‹æ—¥
        workdays = self.df[self.df.index.dayofweek < 5]
        if workdays.empty:
            return None
        
        # è®¡ç®—å¹³å‡æ—¥è´Ÿè·æ›²çº¿
        typical_day = workdays.groupby(workdays.index.hour)[value_col].mean()
        
        # é€‰æ‹©æœ€æ¥è¿‘å¹³å‡å€¼çš„å®é™…æ—¥æœŸ
        daily_avg = workdays.groupby(workdays.index.date)[value_col].mean()
        overall_avg = daily_avg.mean()
        closest_date = daily_avg.sub(overall_avg).abs().idxmin()
        
        actual_day = workdays[workdays.index.date == closest_date]
        
        # è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼
        typical_curve_dict = {str(hour): float(load) for hour, load in typical_day.items()}
        actual_curve_dict = {str(idx): float(val) for idx, val in actual_day[value_col].items()}
        
        return {
            'date': str(closest_date),
            'typical_curve': typical_curve_dict,
            'actual_curve': actual_curve_dict,
            'daily_statistics': {
                'mean': float(actual_day[value_col].mean()),
                'min': float(actual_day[value_col].min()),
                'max': float(actual_day[value_col].max()),
                'std': float(actual_day[value_col].std())
            }
        }
    
    def _get_peak_week(self, value_col):
        """è·å–å³°å€¼å‘¨çš„è´Ÿè·ç‰¹å¾"""
        # æŒ‰å‘¨è®¡ç®—å¹³å‡è´Ÿè·ï¼Œæ‰¾åˆ°å³°å€¼å‘¨
        weekly_avg = self.df.groupby(pd.Grouper(freq='W'))[value_col].mean()
        if weekly_avg.empty:
            return None
        
        peak_week_start = weekly_avg.idxmax()
        peak_week_end = peak_week_start + pd.Timedelta(days=6)
        
        peak_week_data = self.df[peak_week_start:peak_week_end]
        
        # è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼
        weekly_pattern = peak_week_data.groupby(peak_week_data.index.dayofweek)[value_col].mean()
        weekly_pattern_dict = {str(day): float(load) for day, load in weekly_pattern.items()}
        
        return {
            'week_start': str(peak_week_start.date()),
            'week_end': str(peak_week_end.date()),
            'weekly_pattern': weekly_pattern_dict,
            'week_statistics': {
                'mean': float(peak_week_data[value_col].mean()),
                'min': float(peak_week_data[value_col].min()),
                'max': float(peak_week_data[value_col].max()),
                'std': float(peak_week_data[value_col].std())
            }
        }
    
    def _get_seasonal_comparison(self, value_col):
        """è·å–å­£èŠ‚æ€§å¯¹æ¯”åˆ†æ"""
        seasons = {}
        for season_name, months in [('Summer', [6, 7, 8]), ('Winter', [12, 1, 2])]:
            season_data = self.df[self.df.index.month.isin(months)]
            if not season_data.empty:
                daily_pattern = season_data.groupby(season_data.index.hour)[value_col].mean()
                daily_pattern_dict = {str(hour): float(load) for hour, load in daily_pattern.items()}
                
                seasons[season_name] = {
                    'average_load': float(season_data[value_col].mean()),
                    'daily_pattern': daily_pattern_dict,
                    'statistics': {
                        'mean': float(season_data[value_col].mean()),
                        'std': float(season_data[value_col].std()),
                        'min': float(season_data[value_col].min()),
                        'max': float(season_data[value_col].max())
                    }
                }
        
        # è®¡ç®—å­£èŠ‚æ€§å·®å¼‚
        if 'Summer' in seasons and 'Winter' in seasons:
            seasonal_difference = seasons['Summer']['average_load'] - seasons['Winter']['average_load']
            seasons['comparison'] = {
                'difference': float(seasonal_difference),
                'ratio': float(seasons['Summer']['average_load'] / seasons['Winter']['average_load'])
            }
        
        return seasons
    
    def create_cyclical_visualizations(self):
        """åˆ›å»ºå‘¨æœŸæ€§å’Œè¶‹åŠ¿æ€§å¯è§†åŒ–å›¾è¡¨"""
        print("\nğŸ“Š Creating cyclical and trend visualizations...")
        
        value_col = 'value' if 'value' in self.df.columns else self.df.select_dtypes(include=[np.number]).columns[-1]
        
        # çŸ­å‘¨æœŸå¯è§†åŒ–
        self._plot_short_cycle_analysis(value_col)
        
        # é•¿å‘¨æœŸå¯è§†åŒ–
        self._plot_long_cycle_analysis(value_col)
        
        # å…¸å‹æ¡ˆä¾‹å¯è§†åŒ–
        self._plot_case_studies(value_col)
        
        print("âœ… Cyclical and trend visualizations created!")
    
    def _plot_short_cycle_analysis(self, value_col):
        """ç»˜åˆ¶çŸ­å‘¨æœŸåˆ†æå›¾è¡¨"""
        # Set font to support Chinese characters but use English labels
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. æ—¥å†…è´Ÿè·æ›²çº¿
        hourly_data = self.df.groupby(self.df.index.hour)[value_col].agg(['mean', 'std'])
        axes[0, 0].plot(hourly_data.index, hourly_data['mean'], 'b-', linewidth=2, label='Average Load')
        axes[0, 0].fill_between(hourly_data.index, 
                               hourly_data['mean'] - hourly_data['std'],
                               hourly_data['mean'] + hourly_data['std'],
                               alpha=0.3, label='Â±1 Std Dev')
        axes[0, 0].set_title('Daily Load Pattern', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('Hour')
        axes[0, 0].set_ylabel('Load (MW)')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. å‘¨å†…è´Ÿè·æ¨¡å¼
        weekly_data = self.df.groupby(self.df.index.dayofweek)[value_col].mean()
        day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
        axes[0, 1].bar(range(7), weekly_data.values, alpha=0.7, color='skyblue')
        axes[0, 1].set_title('Weekly Load Distribution', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('Day of Week')
        axes[0, 1].set_ylabel('Average Load (MW)')
        axes[0, 1].set_xticks(range(7))
        axes[0, 1].set_xticklabels(day_names, rotation=45)
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. å·¥ä½œæ—¥vså‘¨æœ«å¯¹æ¯”
        df_compare = self.df.copy()
        df_compare['is_weekend'] = df_compare.index.dayofweek >= 5
        df_compare['hour'] = df_compare.index.hour
        comparison = df_compare.groupby(['hour', 'is_weekend'])[value_col].mean().unstack()
        
        if False in comparison.columns and True in comparison.columns:
            axes[1, 0].plot(comparison.index, comparison[False], 'b-', linewidth=2, label='Weekday')
            axes[1, 0].plot(comparison.index, comparison[True], 'r-', linewidth=2, label='Weekend')
        axes[1, 0].set_title('Weekday vs Weekend Load Comparison', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('Hour')
        axes[1, 0].set_ylabel('Average Load (MW)')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. æ—¥è´Ÿè·ç³»æ•°åˆ†å¸ƒ
        daily_load_factor = self.df.groupby(self.df.index.date)[value_col].agg(['mean', 'max'])
        daily_load_factor['load_factor'] = daily_load_factor['mean'] / daily_load_factor['max']
        axes[1, 1].hist(daily_load_factor['load_factor'], bins=30, alpha=0.7, edgecolor='black')
        axes[1, 1].axvline(daily_load_factor['load_factor'].mean(), color='red', linestyle='--', 
                          label=f'Mean: {daily_load_factor["load_factor"].mean():.3f}')
        axes[1, 1].set_title('Daily Load Factor Distribution', fontsize=14, fontweight='bold')
        axes[1, 1].set_xlabel('Load Factor')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'short_cycle_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_long_cycle_analysis(self, value_col):
        """ç»˜åˆ¶é•¿å‘¨æœŸåˆ†æå›¾è¡¨"""
        # Set font to support Chinese characters but use English labels
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. æœˆåº¦è´Ÿè·å˜åŒ–
        monthly_data = self.df.groupby(self.df.index.month)[value_col].agg(['mean', 'std'])
        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
                      'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
        axes[0, 0].plot(monthly_data.index, monthly_data['mean'], 'o-', linewidth=2, markersize=6)
        axes[0, 0].errorbar(monthly_data.index, monthly_data['mean'], yerr=monthly_data['std'], 
                           capsize=5, alpha=0.7)
        axes[0, 0].set_title('Monthly Load Trend', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('Month')
        axes[0, 0].set_ylabel('Average Load (MW)')
        axes[0, 0].set_xticks(range(1, 13))
        axes[0, 0].set_xticklabels(month_names, rotation=45)
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. å­£èŠ‚æ€§è´Ÿè·åˆ†å¸ƒ
        seasonal_data = self.df.copy()
        seasonal_data['season'] = pd.Series(seasonal_data.index.month).apply(self._get_season).values
        season_avg = seasonal_data.groupby('season')[value_col].mean()
        colors = ['lightgreen', 'gold', 'orange', 'lightblue']
        axes[0, 1].bar(season_avg.index, season_avg.values, color=colors, alpha=0.7)
        axes[0, 1].set_title('Seasonal Load Distribution', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('Season')
        axes[0, 1].set_ylabel('Average Load (MW)')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. å¹´åº¦è¶‹åŠ¿
        if len(self.df.index.year.unique()) > 1:
            yearly_data = self.df.groupby(self.df.index.year)[value_col].mean()
            axes[1, 0].plot(yearly_data.index, yearly_data.values, 'o-', linewidth=2, markersize=8)
            
            # æ·»åŠ è¶‹åŠ¿çº¿
            x = np.arange(len(yearly_data))
            coeffs = np.polyfit(x, yearly_data.values, 1)
            trend_line = coeffs[0] * x + coeffs[1]
            axes[1, 0].plot(yearly_data.index, trend_line, '--', color='red', alpha=0.7, 
                           label=f'Trend: {coeffs[0]:.2f} MW/year')
            axes[1, 0].legend()
        else:
            axes[1, 0].text(0.5, 0.5, 'Insufficient data\nfor yearly trend analysis', 
                           ha='center', va='center', transform=axes[1, 0].transAxes, fontsize=12)
        
        axes[1, 0].set_title('Annual Load Trend', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('Year')
        axes[1, 0].set_ylabel('Annual Average Load (MW)')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. é•¿æœŸå˜åŒ–ç³»æ•°
        monthly_cv = self.df.groupby([self.df.index.year, self.df.index.month])[value_col].std() / \
                    self.df.groupby([self.df.index.year, self.df.index.month])[value_col].mean()
        if not monthly_cv.empty:
            axes[1, 1].plot(range(len(monthly_cv)), monthly_cv.values, linewidth=1.5)
            axes[1, 1].axhline(monthly_cv.mean(), color='red', linestyle='--', 
                              label=f'Mean CV: {monthly_cv.mean():.3f}')
            axes[1, 1].legend()
        axes[1, 1].set_title('Load Coefficient of Variation Over Time', fontsize=14, fontweight='bold')
        axes[1, 1].set_xlabel('Time Series')
        axes[1, 1].set_ylabel('Coefficient of Variation')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'long_cycle_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_case_studies(self, value_col):
        """ç»˜åˆ¶å…¸å‹æ¡ˆä¾‹åˆ†æå›¾è¡¨"""
        # Set font to support Chinese characters but use English labels
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. å…¸å‹æ—¥è´Ÿè·æ›²çº¿
        workdays = self.df[self.df.index.dayofweek < 5]
        if not workdays.empty:
            # é€‰æ‹©ä¸€ä¸ªä»£è¡¨æ€§å·¥ä½œæ—¥
            daily_avg = workdays.groupby(workdays.index.date)[value_col].mean()
            overall_avg = daily_avg.mean()
            closest_date = daily_avg.sub(overall_avg).abs().idxmin()
            typical_day = workdays[workdays.index.date == closest_date]
            
            # ç»˜åˆ¶å…¸å‹æ—¥å’Œå¹³å‡æ—¥æ›²çº¿
            avg_hourly = workdays.groupby(workdays.index.hour)[value_col].mean()
            axes[0, 0].plot(typical_day.index.hour, typical_day[value_col], 'b-', 
                           linewidth=2, label=f'Typical Day ({closest_date})')
            axes[0, 0].plot(avg_hourly.index, avg_hourly.values, 'r--', 
                           linewidth=2, label='Weekday Average')
            
        axes[0, 0].set_title('Typical Weekday Load Curve', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('Hour')
        axes[0, 0].set_ylabel('Load (MW)')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. å³°å€¼å‘¨åˆ†æ
        weekly_avg = self.df.groupby(pd.Grouper(freq='W'))[value_col].mean()
        if not weekly_avg.empty:
            peak_week_start = weekly_avg.idxmax()
            peak_week_end = peak_week_start + pd.Timedelta(days=6)
            peak_week_data = self.df[peak_week_start:peak_week_end]
            
            axes[0, 1].plot(peak_week_data.index, peak_week_data[value_col], linewidth=1.5)
            axes[0, 1].set_title(f'Peak Week Load Pattern ({peak_week_start.date()} - {peak_week_end.date()})', 
                                fontsize=14, fontweight='bold')
            axes[0, 1].set_xlabel('Date')
            axes[0, 1].set_ylabel('Load (MW)')
            axes[0, 1].grid(True, alpha=0.3)
        
        # 3. å­£èŠ‚æ€§å¯¹æ¯” (å¤å­£vså†¬å­£)
        summer_data = self.df[self.df.index.month.isin([6, 7, 8])]
        winter_data = self.df[self.df.index.month.isin([12, 1, 2])]
        
        if not summer_data.empty and not winter_data.empty:
            summer_hourly = summer_data.groupby(summer_data.index.hour)[value_col].mean()
            winter_hourly = winter_data.groupby(winter_data.index.hour)[value_col].mean()
            
            axes[1, 0].plot(summer_hourly.index, summer_hourly.values, 'r-', 
                           linewidth=2, label='Summer Average')
            axes[1, 0].plot(winter_hourly.index, winter_hourly.values, 'b-', 
                           linewidth=2, label='Winter Average')
            
        axes[1, 0].set_title('Summer vs Winter Load Comparison', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('Hour')
        axes[1, 0].set_ylabel('Average Load (MW)')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. è´Ÿè·æŒç»­æ›²çº¿
        load_duration = np.sort(self.df[value_col].values)[::-1]
        duration_percent = np.arange(1, len(load_duration) + 1) / len(load_duration) * 100
        
        axes[1, 1].plot(duration_percent, load_duration, linewidth=2)
        axes[1, 1].axhline(self.df[value_col].mean(), color='red', linestyle='--', 
                          label=f'Average Load: {self.df[value_col].mean():.1f} MW')
        axes[1, 1].axvline(50, color='orange', linestyle='--', alpha=0.7, label='Median')
        axes[1, 1].set_title('Load Duration Curve', fontsize=14, fontweight='bold')
        axes[1, 1].set_xlabel('Duration Percentage (%)')
        axes[1, 1].set_ylabel('Load (MW)')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'case_studies.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def create_visualizations(self):
        """Create comprehensive visualizations"""
        print("\nğŸ“Š Creating visualizations...")
        
        # Set font to support Chinese characters but use English labels
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False
        
        # Main value column
        value_col = 'value' if 'value' in self.df.columns else self.df.select_dtypes(include=[np.number]).columns[-1]
        
        # 1. Time series trend plot
        self._plot_time_series_trend(value_col)
        
        # 2. Distribution analysis
        self._plot_distribution_analysis(value_col)
        
        # 3. Seasonality analysis
        self._plot_seasonality_analysis(value_col)
        
        # 4. Correlation heatmap
        self._plot_correlation_heatmap()
        
        # 5. Rolling statistics
        self._plot_rolling_statistics(value_col)
        
        # 6. Outlier detection
        self._plot_outlier_detection(value_col)
        
        # 7. Autocorrelation
        self._plot_autocorrelation(value_col)
        
        # 8. Time series decomposition
        self._plot_decomposition(value_col)
        
        print("âœ… All visualizations created!")
    
    def _plot_time_series_trend(self, value_col):
        """Plot time series trend"""
        plt.figure(figsize=(15, 8))
        
        plt.subplot(2, 1, 1)
        plt.plot(self.df.index, self.df[value_col], alpha=0.7, linewidth=0.8)
        plt.title(f'Time Series Trend - {value_col}', fontsize=14, fontweight='bold')
        plt.ylabel('Value')
        plt.grid(True, alpha=0.3)
        
        plt.subplot(2, 1, 2)
        # Rolling statistics
        rolling_mean = self.df[value_col].rolling(window=24*7).mean()  # Weekly rolling mean
        rolling_std = self.df[value_col].rolling(window=24*7).std()    # Weekly rolling std
        
        plt.plot(self.df.index, self.df[value_col], alpha=0.3, label='Original', linewidth=0.5)
        plt.plot(self.df.index, rolling_mean, 'r-', label='Rolling Mean', linewidth=1.5)
        plt.fill_between(self.df.index, 
                        rolling_mean - rolling_std, 
                        rolling_mean + rolling_std, 
                        alpha=0.2, color='red', label='Â±1 Std')
        plt.title('Time Series with Rolling Statistics', fontsize=14, fontweight='bold')
        plt.ylabel('Value')
        plt.xlabel('Date')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'time_series_trend.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_distribution_analysis(self, value_col):
        """Plot distribution analysis"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        data = self.df[value_col].dropna()
        
        # Histogram
        axes[0, 0].hist(data, bins=50, alpha=0.7, density=True, edgecolor='black')
        axes[0, 0].axvline(data.mean(), color='red', linestyle='--', label=f'Mean: {data.mean():.2f}')
        axes[0, 0].axvline(data.median(), color='green', linestyle='--', label=f'Median: {data.median():.2f}')
        axes[0, 0].set_title('Histogram', fontweight='bold')
        axes[0, 0].set_xlabel('Value')
        axes[0, 0].set_ylabel('Density')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Box plot
        axes[0, 1].boxplot(data, vert=True)
        axes[0, 1].set_title('Box Plot', fontweight='bold')
        axes[0, 1].set_ylabel('Value')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Q-Q plot
        stats.probplot(data, dist="norm", plot=axes[1, 0])
        axes[1, 0].set_title('Q-Q Plot (Normal Distribution)', fontweight='bold')
        axes[1, 0].grid(True, alpha=0.3)
        
        # Cumulative distribution
        sorted_data = np.sort(data)
        cumulative = np.arange(1, len(sorted_data) + 1) / len(sorted_data)
        axes[1, 1].plot(sorted_data, cumulative, linewidth=2)
        axes[1, 1].set_title('Cumulative Distribution Function', fontweight='bold')
        axes[1, 1].set_xlabel('Value')
        axes[1, 1].set_ylabel('Cumulative Probability')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'distribution_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_seasonality_analysis(self, value_col):
        """Plot seasonality analysis"""
        if not isinstance(self.df.index, pd.DatetimeIndex):
            return
            
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        df_with_time = self.df.copy()
        df_with_time['hour'] = df_with_time.index.hour
        df_with_time['day_of_week'] = df_with_time.index.dayofweek
        df_with_time['month'] = df_with_time.index.month
        
        # Hourly pattern
        hourly_mean = df_with_time.groupby('hour')[value_col].mean()
        axes[0, 0].plot(hourly_mean.index, hourly_mean.values, marker='o', linewidth=2)
        axes[0, 0].set_title('Average by Hour of Day', fontweight='bold')
        axes[0, 0].set_xlabel('Hour')
        axes[0, 0].set_ylabel('Average Value')
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].set_xticks(range(0, 24, 2))
        
        # Daily pattern
        daily_mean = df_with_time.groupby('day_of_week')[value_col].mean()
        day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
        axes[0, 1].bar(range(7), daily_mean.values, alpha=0.7)
        axes[0, 1].set_title('Average by Day of Week', fontweight='bold')
        axes[0, 1].set_xlabel('Day of Week')
        axes[0, 1].set_ylabel('Average Value')
        axes[0, 1].set_xticks(range(7))
        axes[0, 1].set_xticklabels(day_names)
        axes[0, 1].grid(True, alpha=0.3)
        
        # Monthly pattern
        monthly_mean = df_with_time.groupby('month')[value_col].mean()
        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
                      'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
        axes[1, 0].plot(monthly_mean.index, monthly_mean.values, marker='s', linewidth=2)
        axes[1, 0].set_title('Average by Month', fontweight='bold')
        axes[1, 0].set_xlabel('Month')
        axes[1, 0].set_ylabel('Average Value')
        axes[1, 0].set_xticks(range(1, 13))
        axes[1, 0].set_xticklabels(month_names, rotation=45)
        axes[1, 0].grid(True, alpha=0.3)
        
        # Heatmap: Hour vs Day of Week
        heatmap_data = df_with_time.pivot_table(values=value_col, index='hour', columns='day_of_week', aggfunc='mean')
        im = axes[1, 1].imshow(heatmap_data.values, cmap='viridis', aspect='auto')
        axes[1, 1].set_title('Heatmap: Hour vs Day of Week', fontweight='bold')
        axes[1, 1].set_xlabel('Day of Week')
        axes[1, 1].set_ylabel('Hour')
        axes[1, 1].set_xticks(range(7))
        axes[1, 1].set_xticklabels(day_names)
        axes[1, 1].set_yticks(range(0, 24, 4))
        plt.colorbar(im, ax=axes[1, 1])
        
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'seasonality_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_correlation_heatmap(self):
        """Plot correlation heatmap"""
        numeric_cols = self.df.select_dtypes(include=[np.number])
        if len(numeric_cols.columns) < 2:
            return
            
        plt.figure(figsize=(10, 8))
        correlation_matrix = numeric_cols.corr()
        
        if HAS_SEABORN:
            mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
            sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', 
                       center=0, square=True, linewidths=0.5)
        else:
            # Fallback to matplotlib imshow
            plt.imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')
            plt.colorbar()
            
            # Add correlation values as text
            for i in range(len(correlation_matrix.columns)):
                for j in range(len(correlation_matrix.columns)):
                    if i != j:  # Don't show diagonal
                        plt.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', 
                               ha='center', va='center', fontsize=8)
            
            plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=45)
            plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)
            
        plt.title('Correlation Heatmap', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'correlation_heatmap.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_rolling_statistics(self, value_col):
        """Plot rolling statistics"""
        plt.figure(figsize=(15, 10))
        
        data = self.df[value_col].dropna()
        
        # Different window sizes
        windows = [24, 24*7, 24*30]  # Daily, weekly, monthly for hourly data
        window_labels = ['24h', '1 week', '1 month']
        
        plt.subplot(3, 1, 1)
        plt.plot(data.index, data.values, alpha=0.3, label='Original', linewidth=0.5)
        for window, label in zip(windows, window_labels):
            if len(data) > window:
                rolling_mean = data.rolling(window=window).mean()
                plt.plot(rolling_mean.index, rolling_mean.values, label=f'Rolling Mean ({label})', linewidth=1.5)
        plt.title('Rolling Mean Comparison', fontweight='bold')
        plt.ylabel('Value')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(3, 1, 2)
        for window, label in zip(windows, window_labels):
            if len(data) > window:
                rolling_std = data.rolling(window=window).std()
                plt.plot(rolling_std.index, rolling_std.values, label=f'Rolling Std ({label})', linewidth=1.5)
        plt.title('Rolling Standard Deviation', fontweight='bold')
        plt.ylabel('Standard Deviation')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(3, 1, 3)
        if len(data) > windows[1]:  # Use weekly window
            rolling_min = data.rolling(window=windows[1]).min()
            rolling_max = data.rolling(window=windows[1]).max()
            rolling_mean = data.rolling(window=windows[1]).mean()
            
            plt.fill_between(data.index, rolling_min, rolling_max, alpha=0.2, label='Min-Max Range')
            plt.plot(rolling_mean.index, rolling_mean.values, 'r-', label='Rolling Mean', linewidth=2)
            plt.plot(data.index, data.values, alpha=0.3, label='Original', linewidth=0.5)
        plt.title('Rolling Min-Max Range with Mean', fontweight='bold')
        plt.ylabel('Value')
        plt.xlabel('Date')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'rolling_statistics.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_outlier_detection(self, value_col):
        """Plot outlier detection"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        data = self.df[value_col].dropna()
        
        # Z-score method
        z_scores = np.abs(stats.zscore(data))
        outliers_zscore = data[z_scores > 3]
        
        axes[0, 0].scatter(range(len(data)), data.values, alpha=0.6, s=1)
        if len(outliers_zscore) > 0:
            outlier_indices = data.index.get_indexer(outliers_zscore.index)
            axes[0, 0].scatter(outlier_indices, outliers_zscore.values, color='red', s=20, label=f'Outliers ({len(outliers_zscore)})')
        axes[0, 0].set_title('Outlier Detection - Z-score Method', fontweight='bold')
        axes[0, 0].set_xlabel('Index')
        axes[0, 0].set_ylabel('Value')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # IQR method
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers_iqr = data[(data < lower_bound) | (data > upper_bound)]
        
        axes[0, 1].scatter(range(len(data)), data.values, alpha=0.6, s=1)
        if len(outliers_iqr) > 0:
            outlier_indices = data.index.get_indexer(outliers_iqr.index)
            axes[0, 1].scatter(outlier_indices, outliers_iqr.values, color='red', s=20, label=f'Outliers ({len(outliers_iqr)})')
        axes[0, 1].axhline(y=upper_bound, color='orange', linestyle='--', alpha=0.8, label='Upper bound')
        axes[0, 1].axhline(y=lower_bound, color='orange', linestyle='--', alpha=0.8, label='Lower bound')
        axes[0, 1].set_title('Outlier Detection - IQR Method', fontweight='bold')
        axes[0, 1].set_xlabel('Index')
        axes[0, 1].set_ylabel('Value')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # Time series plot with outliers highlighted
        axes[1, 0].plot(data.index, data.values, alpha=0.7, linewidth=0.8)
        if len(outliers_zscore) > 0:
            axes[1, 0].scatter(outliers_zscore.index, outliers_zscore.values, 
                             color='red', s=30, alpha=0.8, label=f'Z-score Outliers ({len(outliers_zscore)})')
        axes[1, 0].set_title('Time Series with Outliers (Z-score)', fontweight='bold')
        axes[1, 0].set_xlabel('Date')
        axes[1, 0].set_ylabel('Value')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # Distribution with outliers
        axes[1, 1].hist(data.values, bins=50, alpha=0.7, density=True, edgecolor='black')
        if len(outliers_zscore) > 0:
            axes[1, 1].hist(outliers_zscore.values, bins=20, alpha=0.8, density=True, 
                           color='red', edgecolor='darkred', label=f'Outliers ({len(outliers_zscore)})')
        axes[1, 1].set_title('Distribution with Outliers Highlighted', fontweight='bold')
        axes[1, 1].set_xlabel('Value')
        axes[1, 1].set_ylabel('Density')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'outlier_detection.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_autocorrelation(self, value_col):
        """Plot autocorrelation and partial autocorrelation"""
        data = self.df[value_col].dropna()
        
        if HAS_STATSMODELS and len(data) > 50:
            fig, axes = plt.subplots(1, 2, figsize=(15, 6))
            max_lags = min(50, len(data)//4)
            
            # ACF
            plot_acf(data, lags=max_lags, ax=axes[0], alpha=0.05)
            axes[0].set_title('Autocorrelation Function (ACF)', fontweight='bold')
            axes[0].grid(True, alpha=0.3)
            
            # PACF
            plot_pacf(data, lags=max_lags, ax=axes[1], alpha=0.05)
            axes[1].set_title('Partial Autocorrelation Function (PACF)', fontweight='bold')
            axes[1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(self.plots_dir / 'autocorrelation.png', dpi=300, bbox_inches='tight')
            plt.close()
        else:
            # Simple manual autocorrelation plot
            fig, ax = plt.subplots(1, 1, figsize=(12, 6))
            
            max_lags = min(50, len(data)//4)
            autocorrs = [1.0]  # lag 0 is always 1
            
            for lag in range(1, max_lags):
                if len(data) > lag:
                    corr = np.corrcoef(data[:-lag], data[lag:])[0, 1]
                    autocorrs.append(corr)
                else:
                    break
                    
            ax.stem(range(len(autocorrs)), autocorrs, basefmt=' ')
            ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)
            ax.axhline(y=0.2, color='red', linestyle='--', alpha=0.5, label='Â±0.2 threshold')
            ax.axhline(y=-0.2, color='red', linestyle='--', alpha=0.5)
            ax.set_title('Autocorrelation Function (Manual Implementation)', fontweight='bold')
            ax.set_xlabel('Lag')
            ax.set_ylabel('Autocorrelation')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(self.plots_dir / 'autocorrelation.png', dpi=300, bbox_inches='tight')
            plt.close()
    
    def _plot_decomposition(self, value_col):
        """Plot time series decomposition"""
        if not isinstance(self.df.index, pd.DatetimeIndex) or len(self.df) < 100:
            return
            
        if not HAS_STATSMODELS:
            print("âš ï¸  Statsmodels not available - skipping decomposition plot")
            return
            
        try:
            # Try different seasonal periods
            seasonal_period = 24  # Assume hourly data with daily seasonality
            if len(self.df) < 2 * seasonal_period:
                seasonal_period = len(self.df) // 4
                
            if seasonal_period < 2:
                return
                
            decomposition = seasonal_decompose(self.df[value_col].dropna(), 
                                             model='additive', 
                                             period=seasonal_period)
            
            fig, axes = plt.subplots(4, 1, figsize=(15, 12))
            
            # Original
            decomposition.observed.plot(ax=axes[0], title='Original Time Series')
            axes[0].grid(True, alpha=0.3)
            
            # Trend
            decomposition.trend.plot(ax=axes[1], title='Trend Component', color='orange')
            axes[1].grid(True, alpha=0.3)
            
            # Seasonal
            decomposition.seasonal.plot(ax=axes[2], title='Seasonal Component', color='green')
            axes[2].grid(True, alpha=0.3)
            
            # Residual
            decomposition.resid.plot(ax=axes[3], title='Residual Component', color='red')
            axes[3].grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(self.plots_dir / 'decomposition.png', dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"âš ï¸  Could not create decomposition plot: {e}")
    
    def generate_comprehensive_report(self):
        """Generate a comprehensive markdown report in Chinese"""
        print("\nğŸ“ ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š...")
        
        report_content = f"""# è´Ÿè·æ•°æ®æ·±åº¦åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´:** {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}

**æ•°æ®é›†:** {self.data_path}

## æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘Šå¯¹ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹çš„è´Ÿè·æ•°æ®è¿›è¡Œäº†å…¨é¢æ·±å…¥çš„åˆ†æã€‚åˆ†ææ¶µç›–åŸºæœ¬ç»Ÿè®¡ç‰¹å¾ã€æ—¶é—´åºåˆ—ç‰¹æ€§ã€æ•°æ®è´¨é‡è¯„ä¼°ã€æ¨¡å¼è¯†åˆ«å’Œå¯è§†åŒ–å±•ç¤ºã€‚è¯¥æ•°æ®é›†è¡¨ç°å‡ºæ˜æ˜¾çš„æ—¶é—´è§„å¾‹æ€§å’Œå­£èŠ‚æ€§ç‰¹å¾ï¼Œä¸ºç”µåŠ›è´Ÿè·é¢„æµ‹æä¾›äº†é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚

## 1. æ•°æ®é›†æ¦‚è§ˆ

- **æ•°æ®è§„æ¨¡:** {self.df.shape[0]:,} æ¡è®°å½• Ã— {self.df.shape[1]} ä¸ªç‰¹å¾
- **ç‰¹å¾åˆ—:** {', '.join(self.df.columns)}
- **æ—¶é—´è·¨åº¦:** {self.df.index.min()} è‡³ {self.df.index.max()}
- **æŒç»­æ—¶é•¿:** {(self.df.index.max() - self.df.index.min()).days:,} å¤©
- **æ•°æ®é¢‘ç‡:** 5åˆ†é’Ÿé—´éš”ï¼ˆæ¯å°æ—¶12ä¸ªæ•°æ®ç‚¹ï¼‰

### æ—¶é—´åºåˆ—æ•´ä½“è¶‹åŠ¿

![æ—¶é—´åºåˆ—è¶‹åŠ¿å›¾](../plots/time_series_trend.png)

ä¸Šå›¾å±•ç¤ºäº†è´Ÿè·æ•°æ®çš„æ•´ä½“æ—¶é—´åºåˆ—è¶‹åŠ¿ã€‚ä¸ŠåŠéƒ¨åˆ†æ˜¾ç¤ºåŸå§‹æ—¶é—´åºåˆ—ï¼Œå¯ä»¥è§‚å¯Ÿåˆ°ï¼š
- æ•°æ®å…·æœ‰æ˜æ˜¾çš„å‘¨æœŸæ€§æ³¢åŠ¨
- æ•´ä½“å‘ˆç°è½»å¾®ä¸Šå‡è¶‹åŠ¿
- å­˜åœ¨æ˜æ˜¾çš„å­£èŠ‚æ€§å˜åŒ–æ¨¡å¼

ä¸‹åŠéƒ¨åˆ†æ˜¾ç¤ºæ»šåŠ¨ç»Ÿè®¡ç‰¹å¾ï¼š
- çº¢çº¿ä¸º7å¤©æ»šåŠ¨å‡å€¼ï¼Œåæ˜ ä¸­æœŸè¶‹åŠ¿
- é˜´å½±åŒºåŸŸä¸ºÂ±1æ ‡å‡†å·®èŒƒå›´ï¼Œæ˜¾ç¤ºæ•°æ®å˜å¼‚æ€§
- å¯ä»¥çœ‹å‡ºæ•°æ®çš„æ³¢åŠ¨æ€§ç›¸å¯¹ç¨³å®š

## 2. æ•°æ®è´¨é‡è¯„ä¼°

"""
        
        # Add quality assessment results
        if 'data_quality_assessment' in self.analysis_results:
            quality_data = self.analysis_results['data_quality_assessment']
            report_content += f"""
### æ•°æ®è´¨é‡ç»¼åˆè¯„åˆ†: {quality_data.get('overall_quality_score', 'N/A'):.1f}/100

### ç¼ºå¤±å€¼åˆ†æ
"""
            for col, missing_count in quality_data.get('missing_patterns', {}).items():
                missing_pct = (missing_count / len(self.df)) * 100
                report_content += f"- **{col}:** {missing_count} ä¸ªç¼ºå¤±å€¼ ({missing_pct:.2f}%)\n"
            
            report_content += "\n### æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥\n"
            consistency = quality_data.get('consistency_checks', {})
            for col, checks in consistency.items():
                report_content += f"\n**{col}å­—æ®µ:**\n"
                report_content += f"- IQRæ–¹æ³•å¼‚å¸¸å€¼: {checks.get('outliers_iqr', 'N/A')} ä¸ª\n"
                report_content += f"- Z-scoreæ–¹æ³•å¼‚å¸¸å€¼: {checks.get('outliers_zscore', 'N/A')} ä¸ª\n"
        
        # Add basic statistics
        if 'basic_statistics' in self.analysis_results:
            stats_data = self.analysis_results['basic_statistics']
            report_content += "\n## 3. ç»Ÿè®¡ç‰¹å¾åˆ†æ\n"
            
            report_content += "\n### åˆ†å¸ƒç‰¹å¾å¯è§†åŒ–\n\n![åˆ†å¸ƒåˆ†æå›¾](../plots/distribution_analysis.png)\n\n"
            report_content += """åˆ†å¸ƒåˆ†æå›¾åŒ…å«å››ä¸ªå…³é”®è§†å›¾ï¼š
1. **ç›´æ–¹å›¾**: æ˜¾ç¤ºæ•°æ®åˆ†å¸ƒå½¢çŠ¶ï¼Œçº¢è‰²è™šçº¿ä¸ºå‡å€¼ï¼Œç»¿è‰²è™šçº¿ä¸ºä¸­ä½æ•°
2. **ç®±çº¿å›¾**: å±•ç¤ºå››åˆ†ä½æ•°åˆ†å¸ƒå’Œå¼‚å¸¸å€¼è¯†åˆ«
3. **Q-Qå›¾**: æ£€éªŒæ•°æ®æ˜¯å¦ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼ˆç‚¹è¶Šæ¥è¿‘ç›´çº¿è¶Šç¬¦åˆæ­£æ€åˆ†å¸ƒï¼‰
4. **ç´¯ç§¯åˆ†å¸ƒå‡½æ•°**: æ˜¾ç¤ºæ•°æ®çš„ç´¯ç§¯æ¦‚ç‡åˆ†å¸ƒ

"""
            
            for col, stats in stats_data.get('descriptive_stats', {}).items():
                report_content += f"\n### {col}å­—æ®µç»Ÿè®¡æŒ‡æ ‡\n"
                
                # å®‰å…¨çš„æ•°å­—æ ¼å¼åŒ–å‡½æ•°
                def safe_format(value, format_str=".4f"):
                    if isinstance(value, (int, float)) and not pd.isna(value):
                        if format_str == ",":
                            return f"{value:,}"
                        else:
                            return f"{value:,.4f}"
                    else:
                        return "N/A"
                
                report_content += f"- **æ•°æ®é‡:** {safe_format(stats.get('count'), ',')}\n"
                report_content += f"- **å‡å€¼:** {safe_format(stats.get('mean'))}\n"
                report_content += f"- **æ ‡å‡†å·®:** {safe_format(stats.get('std'))}\n"
                report_content += f"- **æœ€å°å€¼:** {safe_format(stats.get('min'))}\n"
                report_content += f"- **ç¬¬25ç™¾åˆ†ä½:** {safe_format(stats.get('q25'))}\n"
                report_content += f"- **ä¸­ä½æ•°:** {safe_format(stats.get('median'))}\n"
                report_content += f"- **ç¬¬75ç™¾åˆ†ä½:** {safe_format(stats.get('q75'))}\n"
                report_content += f"- **æœ€å¤§å€¼:** {safe_format(stats.get('max'))}\n"
                report_content += f"- **ååº¦:** {safe_format(stats.get('skewness'))} (æ­£å€¼è¡¨ç¤ºå³å)\n"
                report_content += f"- **å³°åº¦:** {safe_format(stats.get('kurtosis'))} (æ­£å€¼è¡¨ç¤ºåšå°¾åˆ†å¸ƒ)\n"
                
                # æ·»åŠ ç»Ÿè®¡è§£é‡Š
                skew = stats.get('skewness', 0)
                kurt = stats.get('kurtosis', 0)
                
                report_content += "\n**ç»Ÿè®¡ç‰¹å¾è§£é‡Š:**\n"
                if abs(skew) < 0.5:
                    report_content += "- æ•°æ®åˆ†å¸ƒåŸºæœ¬å¯¹ç§°\n"
                elif skew > 0.5:
                    report_content += "- æ•°æ®å‘ˆç°æ˜æ˜¾å³ååˆ†å¸ƒï¼ˆé«˜å€¼è¾ƒå¤šï¼‰\n"
                else:
                    report_content += "- æ•°æ®å‘ˆç°æ˜æ˜¾å·¦ååˆ†å¸ƒï¼ˆä½å€¼è¾ƒå¤šï¼‰\n"
                    
                if abs(kurt) < 0.5:
                    report_content += "- æ•°æ®åˆ†å¸ƒæ¥è¿‘æ­£æ€åˆ†å¸ƒçš„å³°åº¦\n"
                elif kurt > 0.5:
                    report_content += "- æ•°æ®åˆ†å¸ƒå…·æœ‰åšå°¾ç‰¹å¾ï¼ˆæå€¼è¾ƒå¤šï¼‰\n"
                else:
                    report_content += "- æ•°æ®åˆ†å¸ƒå…·æœ‰è–„å°¾ç‰¹å¾ï¼ˆæå€¼è¾ƒå°‘ï¼‰\n"
        
        # Add time series analysis
        if 'time_series_analysis' in self.analysis_results:
            ts_data = self.analysis_results['time_series_analysis']
            report_content += "\n## 4. æ—¶é—´åºåˆ—ç‰¹æ€§åˆ†æ\n"
            
            # è¶‹åŠ¿åˆ†æ
            trend = ts_data.get('trend_analysis', {})
            
            # å®‰å…¨æ ¼å¼åŒ–è¶‹åŠ¿æ•°æ®
            def safe_trend_format(value, format_type='float'):
                if isinstance(value, (int, float)) and not pd.isna(value):
                    if format_type == 'slope':
                        return f"{value:,.6f}"
                    elif format_type == 'percent':
                        return f"{value:,.4f}"
                    else:
                        return f"{value:,.6f}"
                else:
                    return "N/A"
            
            slope_str = safe_trend_format(trend.get('slope'), 'slope')
            r_squared_str = safe_trend_format(trend.get('r_squared'), 'percent')
            p_value_str = safe_trend_format(trend.get('p_value'), 'percent')
            
            report_content += f"""
### è¶‹åŠ¿åˆ†æ
- **è¶‹åŠ¿æ–¹å‘:** {trend.get('trend_direction', 'N/A')}
- **æ–œç‡ç³»æ•°:** {slope_str}
- **å†³å®šç³»æ•° (RÂ²):** {r_squared_str}
- **ç»Ÿè®¡æ˜¾è‘—æ€§ (på€¼):** {p_value_str}

**è¶‹åŠ¿è§£é‡Š:** """
            
            slope = trend.get('slope', 0)
            r_squared = trend.get('r_squared', 0)
            p_value = trend.get('p_value', 1)
            
            if abs(slope) < 0.001:
                report_content += "æ•°æ®æ•´ä½“å‘ˆç°å¹³ç¨³è¶‹åŠ¿ï¼Œæ— æ˜æ˜¾ä¸Šå‡æˆ–ä¸‹é™ã€‚"
            elif slope > 0:
                report_content += f"æ•°æ®å‘ˆç°{trend.get('trend_direction', 'ä¸Šå‡')}è¶‹åŠ¿ã€‚"
            else:
                report_content += f"æ•°æ®å‘ˆç°{trend.get('trend_direction', 'ä¸‹é™')}è¶‹åŠ¿ã€‚"
            
            if r_squared > 0.1:
                report_content += f"è¶‹åŠ¿çš„è§£é‡ŠåŠ›åº¦ä¸º{r_squared*100:.1f}%ï¼Œè¶‹åŠ¿ç›¸å¯¹æ˜æ˜¾ã€‚"
            else:
                report_content += f"è¶‹åŠ¿çš„è§£é‡ŠåŠ›åº¦ä»…ä¸º{r_squared*100:.1f}%ï¼Œè¶‹åŠ¿è¾ƒå¼±ã€‚"
            
            if p_value < 0.05:
                report_content += "è¶‹åŠ¿åœ¨ç»Ÿè®¡å­¦ä¸Šæ˜¾è‘—ã€‚"
            else:
                report_content += "è¶‹åŠ¿åœ¨ç»Ÿè®¡å­¦ä¸Šä¸æ˜¾è‘—ã€‚"
            
            # è‡ªç›¸å…³åˆ†æ
            autocorr = ts_data.get('autocorrelation', {})
            report_content += f"""

### è‡ªç›¸å…³åˆ†æ
![è‡ªç›¸å…³å›¾](../plots/autocorrelation.png)

- **å‰10ä¸ªæ»åæœŸçš„è‡ªç›¸å…³ç³»æ•°:** {autocorr.get('lags_1_to_10', ['N/A'])}
- **æ˜¾è‘—è‡ªç›¸å…³çš„æ»åæœŸæ•°é‡:** {autocorr.get('significant_autocorr_count', 'N/A')}

è‡ªç›¸å…³åˆ†ææ˜¾ç¤ºäº†æ•°æ®çš„æ—¶é—´ä¾èµ–æ€§ã€‚é«˜è‡ªç›¸å…³ç³»æ•°è¡¨æ˜å½“å‰å€¼ä¸å†å²å€¼å¯†åˆ‡ç›¸å…³ï¼Œè¿™ä¸ºæ—¶é—´åºåˆ—é¢„æµ‹æä¾›äº†é‡è¦ä¿¡æ¯ã€‚
"""
            
            # å­£èŠ‚æ€§åˆ†æ
            if 'seasonality' in ts_data:
                seasonality = ts_data['seasonality']
                
                # å®‰å…¨çš„æ•°å­—æ ¼å¼åŒ–å‡½æ•°
                def safe_format_num(value, decimals=4):
                    if isinstance(value, (int, float)) and not pd.isna(value):
                        return f"{value:.{decimals}f}"
                    else:
                        return "N/A"
                
                daily_strength = safe_format_num(seasonality.get('period_24', 'N/A'))
                weekly_strength = safe_format_num(seasonality.get('period_168', 'N/A'))
                yearly_strength = safe_format_num(seasonality.get('period_8760', 'N/A'))
                
                report_content += f"""

### å­£èŠ‚æ€§ç‰¹å¾æ£€æµ‹
- **æ—¥å†…å‘¨æœŸå¼ºåº¦:** {daily_strength}
- **å‘¨å†…å‘¨æœŸå¼ºåº¦:** {weekly_strength}
- **å¹´å†…å‘¨æœŸå¼ºåº¦:** {yearly_strength}

![å­£èŠ‚æ€§åˆ†æå›¾](../plots/seasonality_analysis.png)

å­£èŠ‚æ€§åˆ†æå›¾å±•ç¤ºäº†å››ä¸ªç»´åº¦çš„å‘¨æœŸæ€§æ¨¡å¼ï¼š
1. **å°æ—¶æ¨¡å¼**: æ˜¾ç¤ºä¸€å¤©24å°æ—¶å†…çš„è´Ÿè·å˜åŒ–è§„å¾‹
2. **æ˜ŸæœŸæ¨¡å¼**: å±•ç¤ºä¸€å‘¨7å¤©çš„è´Ÿè·å·®å¼‚ï¼ˆå·¥ä½œæ—¥vså‘¨æœ«ï¼‰
3. **æœˆä»½æ¨¡å¼**: åæ˜ å…¨å¹´12ä¸ªæœˆçš„å­£èŠ‚æ€§å˜åŒ–
4. **çƒ­åŠ›å›¾**: å°æ—¶ä¸æ˜ŸæœŸçš„äº¤å‰åˆ†æï¼Œè¯†åˆ«ç²¾ç»†æ—¶é—´æ¨¡å¼

å­£èŠ‚æ€§å¼ºåº¦è¶Šæ¥è¿‘1ï¼Œè¡¨æ˜è¯¥æ—¶é—´å°ºåº¦çš„å‘¨æœŸæ€§è¶Šæ˜æ˜¾ã€‚"""
        
        # Add pattern analysis
        if 'pattern_analysis' in self.analysis_results:
            pattern_data = self.analysis_results['pattern_analysis']
            report_content += "\n## 5. æ¨¡å¼è¯†åˆ«ä¸å¼‚å¸¸æ£€æµ‹\n"
            
            # æ—¶é—´æ¨¡å¼
            if 'time_patterns' in pattern_data:
                patterns = pattern_data['time_patterns']
                report_content += f"""
### æ—¶é—´æ¨¡å¼åˆ†æ
- **å³°å€¼æ—¶æ®µ:** {patterns.get('peak_hour', 'N/A')}:00
- **ä½è°·æ—¶æ®µ:** {patterns.get('low_hour', 'N/A')}:00
- **å³°å€¼å·¥ä½œæ—¥:** {['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥'][patterns.get('peak_day', 0)]}
- **ä½è°·å·¥ä½œæ—¥:** {['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥'][patterns.get('low_day', 6)]}

"""
            elif 'periodic_patterns' in pattern_data:
                patterns = pattern_data['periodic_patterns']
                if 'hourly' in patterns:
                    hourly = patterns['hourly']
                    report_content += f"""
### æ—¶é—´æ¨¡å¼åˆ†æ - å°æ—¶æ¨¡å¼
- **å³°å€¼æ—¶æ®µ:** {hourly.get('peak_hour', 'N/A')}:00
- **ä½è°·æ—¶æ®µ:** {hourly.get('low_hour', 'N/A')}:00

"""
                
                if 'weekly' in patterns:
                    weekly = patterns['weekly']
                    days = ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥']
                    peak_day = days[weekly.get('peak_day', 0)] if weekly.get('peak_day', 0) < 7 else 'N/A'
                    low_day = days[weekly.get('low_day', 0)] if weekly.get('low_day', 0) < 7 else 'N/A'
                    report_content += f"""
### æ—¶é—´æ¨¡å¼åˆ†æ - å‘¨æ¨¡å¼
- **å³°å€¼å·¥ä½œæ—¥:** {peak_day}
- **ä½è°·å·¥ä½œæ—¥:** {low_day}

"""
            
            # æå€¼åˆ†æ
            if 'extreme_values' in pattern_data:
                extreme = pattern_data['extreme_values']
                report_content += f"""
### æå€¼åˆ†æ
**æœ€é«˜è´Ÿè·è®°å½• (Top 5):**
"""
                for i, val in enumerate(extreme.get('top_5_values', []), 1):
                    report_content += f"{i}. {val:,.2f}\n"
                
                report_content += f"""
**æœ€ä½è´Ÿè·è®°å½• (Bottom 5):**
"""
                for i, val in enumerate(extreme.get('bottom_5_values', []), 1):
                    report_content += f"{i}. {val:,.2f}\n"
                
                extreme_dates = extreme.get('extreme_dates', {})
                report_content += f"""
**æå€¼å‘ç”Ÿæ—¶é—´:**
- æœ€å¤§å€¼æ—¶é—´: {extreme_dates.get('max_date', 'N/A')}
- æœ€å°å€¼æ—¶é—´: {extreme_dates.get('min_date', 'N/A')}
"""
            
            # å¼‚å¸¸æ£€æµ‹
            report_content += """
### å¼‚å¸¸å€¼æ£€æµ‹

![å¼‚å¸¸æ£€æµ‹å›¾](../plots/outlier_detection.png)

å¼‚å¸¸æ£€æµ‹é‡‡ç”¨ä¸¤ç§æ–¹æ³•ï¼š
1. **Z-scoreæ–¹æ³•**: åŸºäºæ ‡å‡†åŒ–å¾—åˆ†ï¼Œ|Z|>3è§†ä¸ºå¼‚å¸¸
2. **IQRæ–¹æ³•**: åŸºäºå››åˆ†ä½è·ï¼Œè¶…å‡ºQ1-1.5*IQRæˆ–Q3+1.5*IQRè§†ä¸ºå¼‚å¸¸

å›¾ä¸­æ˜¾ç¤ºäº†ä¸åŒæ–¹æ³•è¯†åˆ«çš„å¼‚å¸¸ç‚¹åˆ†å¸ƒå’Œæ—¶é—´åºåˆ—ä¸­çš„å¼‚å¸¸ä½ç½®ã€‚
"""
        
        # æ»šåŠ¨ç»Ÿè®¡åˆ†æ
        report_content += """
## 6. æ»šåŠ¨ç»Ÿè®¡åˆ†æ

![æ»šåŠ¨ç»Ÿè®¡å›¾](../plots/rolling_statistics.png)

æ»šåŠ¨ç»Ÿè®¡åˆ†æå±•ç¤ºäº†ä¸åŒæ—¶é—´çª—å£ä¸‹çš„ç»Ÿè®¡ç‰¹å¾å˜åŒ–ï¼š
1. **æ»šåŠ¨å‡å€¼æ¯”è¾ƒ**: 24å°æ—¶ã€1å‘¨ã€1ä¸ªæœˆçª—å£çš„ç§»åŠ¨å¹³å‡
2. **æ»šåŠ¨æ ‡å‡†å·®**: åæ˜ æ•°æ®åœ¨ä¸åŒæ—¶æœŸçš„æ³¢åŠ¨æ€§
3. **æ»šåŠ¨èŒƒå›´**: æœ€å¤§å€¼ã€æœ€å°å€¼å’Œå‡å€¼çš„åŠ¨æ€å˜åŒ–

è¿™äº›æŒ‡æ ‡æœ‰åŠ©äºè¯†åˆ«æ•°æ®çš„æ—¶å˜ç‰¹å¾å’Œç¨³å®šæ€§ã€‚
"""
        
        # ç›¸å…³æ€§åˆ†æ
        report_content += """
## 7. å¯è§†åŒ–å›¾è¡¨è¯´æ˜

æœ¬åˆ†æç”Ÿæˆäº†6ç±»ä¸“ä¸šå¯è§†åŒ–å›¾è¡¨ï¼Œè¯¦ç»†è¯´æ˜å¦‚ä¸‹ï¼š

### 7.1 æ—¶é—´åºåˆ—è¶‹åŠ¿å›¾ 
![æ—¶é—´åºåˆ—è¶‹åŠ¿å›¾](../plots/time_series_trend.png)

### 7.2 åˆ†å¸ƒåˆ†æå›¾
![åˆ†å¸ƒåˆ†æå›¾](../plots/distribution_analysis.png)

### 7.3 å­£èŠ‚æ€§åˆ†æå›¾
![å­£èŠ‚æ€§åˆ†æå›¾](../plots/seasonality_analysis.png)

### 8.5 æ»šåŠ¨ç»Ÿè®¡å›¾
![æ»šåŠ¨ç»Ÿè®¡å›¾](../plots/rolling_statistics.png)

### 8.6 å¼‚å¸¸æ£€æµ‹å›¾
![å¼‚å¸¸æ£€æµ‹å›¾](../plots/outlier_detection.png)

### 7.6 è‡ªç›¸å…³åˆ†æå›¾
![è‡ªç›¸å…³åˆ†æå›¾](../plots/autocorrelation.png)

## 8. å…³é”®å‘ç°ä¸å»ºè®®

### 8.1 æ•°æ®è´¨é‡è¯„ä¼°ç»“è®º
- **æ•°æ®å®Œæ•´æ€§**: æ•°æ®é›†ç»“æ„å®Œæ•´ï¼Œé€‚åˆç”¨äºæ—¶é—´åºåˆ—åˆ†æ
- **ç¼ºå¤±å€¼å¤„ç†**: éœ€è¦åœ¨å»ºæ¨¡å‰é€‚å½“å¤„ç†ç¼ºå¤±å€¼æ¨¡å¼
- **å¼‚å¸¸å€¼è¯†åˆ«**: å·²è¯†åˆ«å¼‚å¸¸å€¼ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥å…¶äº§ç”ŸåŸå› 

### 9.2 æ—¶é—´åºåˆ—ç‰¹æ€§æ€»ç»“
- **å‘¨æœŸæ€§æ¨¡å¼**: æ•°æ®å±•ç°å‡ºæ¸…æ™°çš„å¤šå°ºåº¦æ—¶é—´æ¨¡å¼
- **å­£èŠ‚æ€§ç‰¹å¾**: åœ¨å°æ—¶ã€æ—¥ã€å‘¨ç­‰å¤šä¸ªæ—¶é—´å°ºåº¦ä¸Šå­˜åœ¨å­£èŠ‚æ€§
- **è¶‹åŠ¿åˆ†æ**: é•¿æœŸè¶‹åŠ¿åˆ†æä¸ºé¢„æµ‹æ¨¡å‹æä¾›é‡è¦å‚è€ƒ

### 9.3 ç”µåŠ›è´Ÿè·ç‰¹å¾æ´å¯Ÿ
1. **æ—¥å†…æ¨¡å¼**: å…¸å‹çš„ç”µåŠ›è´Ÿè·æ—¥å†…å˜åŒ–æ›²çº¿ï¼Œå³°è°·å·®å¼‚æ˜æ˜¾
2. **å‘¨å†…æ¨¡å¼**: å·¥ä½œæ—¥ä¸å‘¨æœ«è´Ÿè·æ¨¡å¼å­˜åœ¨æ˜¾è‘—å·®å¼‚
3. **å­£èŠ‚æ¨¡å¼**: å¹´å†…å­£èŠ‚å˜åŒ–åæ˜ ç”¨ç”µä¹ æƒ¯å’Œæ°”å€™å½±å“
4. **è´Ÿè·æ°´å¹³**: æ•´ä½“è´Ÿè·æ°´å¹³åœ¨åˆç†èŒƒå›´å†…ï¼Œå˜å¼‚ç³»æ•°é€‚ä¸­

### 9.4 çŸ­å‘¨æœŸè¶‹åŠ¿åˆ†æ

![çŸ­å‘¨æœŸåˆ†æ](../plots/short_cycle_analysis.png)

#### 9.4.1 æ—¥å†…è´Ÿè·å˜åŒ–è§„å¾‹
åŸºäºå°æ—¶çº§è´Ÿè·æ•°æ®çš„åˆ†ææ˜¾ç¤ºäº†æ˜æ˜¾çš„æ—¥å†…å˜åŒ–æ¨¡å¼ï¼š
- **å³°å€¼æ—¶æ®µ**: é€šå¸¸å‡ºç°åœ¨ç”¨ç”µéœ€æ±‚è¾ƒé«˜çš„æ—¶æ®µï¼Œåæ˜ äº†è´Ÿè·çš„æ—¥å¸¸è§„å¾‹
- **è°·å€¼æ—¶æ®µ**: ä¸€èˆ¬åœ¨å¤œé—´æˆ–ç”¨ç”µéœ€æ±‚è¾ƒä½çš„æ—¶æ®µï¼Œè´Ÿè·å¤„äºæœ€ä½æ°´å¹³
- **å³°è°·å·®ç‡**: åæ˜ äº†è´Ÿè·çš„æ—¥å†…å˜åŒ–å¹…åº¦ï¼Œæ˜¯ç”µåŠ›ç³»ç»Ÿè¿è¡Œçš„é‡è¦æŒ‡æ ‡
- **è´Ÿè·ç³»æ•°**: è¡¨å¾è´Ÿè·çš„å¹³å‡åˆ©ç”¨æ•ˆç‡ï¼Œå½±å“ç”µç½‘çš„ç»æµè¿è¡Œ

#### 9.4.2 å‘¨å†…è´Ÿè·åˆ†å¸ƒç‰¹å¾
å·¥ä½œæ—¥ä¸å‘¨æœ«çš„è´Ÿè·æ¨¡å¼å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼š
- å·¥ä½œæ—¥è´Ÿè·ä¸€èˆ¬è¾ƒé«˜ï¼Œåæ˜ äº†å•†ä¸šå’Œå·¥ä¸šç”¨ç”µçš„é›†ä¸­æ€§
- å‘¨æœ«è´Ÿè·ç›¸å¯¹è¾ƒä½ï¼Œä¸»è¦ç”±æ°‘ç”¨ç”µè´Ÿè·ä¸»å¯¼
- å‘¨å†…è´Ÿè·åˆ†å¸ƒåæ˜ äº†ç»æµæ´»åŠ¨å’Œç”Ÿæ´»ä¹ æƒ¯çš„å‘¨æœŸæ€§ç‰¹å¾

### 9.5 é•¿å‘¨æœŸè¶‹åŠ¿åˆ†æ

![é•¿å‘¨æœŸåˆ†æ](../plots/long_cycle_analysis.png)

#### 9.5.1 å­£èŠ‚æ€§è´Ÿè·ç‰¹å¾
å­£èŠ‚æ€§åˆ†ææ­ç¤ºäº†è´Ÿè·éšå­£èŠ‚å˜åŒ–çš„è§„å¾‹ï¼š
- **å­£èŠ‚æ€§æ¨¡å¼**: ä¸åŒå­£èŠ‚çš„å¹³å‡è´Ÿè·æ°´å¹³åæ˜ äº†æ°”å€™å¯¹ç”¨ç”µéœ€æ±‚çš„å½±å“
- **æœˆåº¦å˜åŒ–**: æœˆåº¦è´Ÿè·å˜åŒ–è¶‹åŠ¿æ˜¾ç¤ºäº†æ›´ç»†è‡´çš„å­£èŠ‚æ€§ç‰¹å¾
- **å¹´åº¦è¶‹åŠ¿**: å¹´åº¦è´Ÿè·å¢é•¿è¶‹åŠ¿åæ˜ äº†ç»æµå‘å±•å’Œç”¨ç”µéœ€æ±‚çš„å˜åŒ–

#### 9.5.2 é•¿æœŸå˜åŒ–ç‰¹å¾
- **å¹´åº¦å¢é•¿**: è´Ÿè·çš„å¹´åº¦å¢é•¿ç‡åæ˜ äº†åœ°åŒºç»æµå‘å±•çŠ¶å†µ
- **å˜å¼‚ç³»æ•°**: è´Ÿè·å˜å¼‚ç³»æ•°çš„å˜åŒ–è¡¨å¾äº†è´Ÿè·é¢„æµ‹çš„éš¾æ˜“ç¨‹åº¦
- **è¶‹åŠ¿è¯†åˆ«**: é•¿æœŸè¶‹åŠ¿åˆ†æä¸ºæœªæ¥è§„åˆ’æä¾›é‡è¦å‚è€ƒ

### 9.6 å…¸å‹æ¡ˆä¾‹åˆ†æ

![å…¸å‹æ¡ˆä¾‹åˆ†æ](../plots/case_studies.png)

#### 9.6.1 å…¸å‹æ—¥è´Ÿè·åˆ†æ
é€‰æ‹©ä»£è¡¨æ€§çš„å·¥ä½œæ—¥è¿›è¡Œæ·±åº¦åˆ†æï¼š
- **è´Ÿè·æ›²çº¿**: å…¸å‹æ—¥è´Ÿè·æ›²çº¿å±•ç°äº†æ ‡å‡†çš„ç”¨ç”µæ¨¡å¼
- **è´Ÿè·ç‰¹å¾**: æ—¥æœ€å¤§ã€æœ€å°è´Ÿè·åŠå…¶å‡ºç°æ—¶é—´åæ˜ äº†ç”¨ç”µè§„å¾‹
- **å˜åŒ–ç‰¹å¾**: æ—¥å†…è´Ÿè·å˜åŒ–çš„å¹…åº¦å’Œè§„å¾‹æ€§

#### 9.6.2 å³°å€¼å‘¨è´Ÿè·ç‰¹å¾
å³°å€¼å‘¨åˆ†ææä¾›äº†ç³»ç»Ÿé«˜è´Ÿè·è¿è¡Œçš„å‚è€ƒï¼š
- **è´Ÿè·æ°´å¹³**: å³°å€¼å‘¨çš„è´Ÿè·æ°´å¹³ä»£è¡¨äº†ç³»ç»Ÿçš„é«˜éœ€æ±‚çŠ¶æ€
- **å˜åŒ–æ¨¡å¼**: å³°å€¼å‘¨å†…çš„è´Ÿè·å˜åŒ–æ¨¡å¼
- **è¿è¡Œç‰¹å¾**: é«˜è´Ÿè·æ—¶æœŸçš„ç³»ç»Ÿè¿è¡Œç‰¹å¾

#### 9.6.3 å­£èŠ‚æ€§å¯¹æ¯”åˆ†æ
å¤å†¬è´Ÿè·å¯¹æ¯”åˆ†æï¼š
- **å­£èŠ‚å·®å¼‚**: å¤å­£å’Œå†¬å­£è´Ÿè·çš„å·®å¼‚åæ˜ äº†æ°”å€™å¯¹ç”¨ç”µçš„å½±å“
- **è´Ÿè·ç‰¹å¾**: ä¸åŒå­£èŠ‚çš„æ—¥å†…è´Ÿè·æ›²çº¿ç‰¹å¾
- **ç”¨ç”µæ¨¡å¼**: å­£èŠ‚æ€§ç”¨ç”µæ¨¡å¼çš„å·®å¼‚åˆ†æ
"""
        
        # Save the report
        report_path = self.reports_dir / 'comprehensive_analysis_report.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ… Comprehensive report saved to: {report_path}")
        return report_content
    
    def run_complete_analysis(self):
        """Run the complete analysis pipeline"""
        print("ğŸš€ Starting comprehensive load_data analysis...")
        print("=" * 60)
        
        # Load data
        if not self.load_data():
            return False
        
        # Perform all analyses
        self.basic_statistics()
        self.time_series_analysis()
        self.data_quality_assessment()
        self.pattern_analysis()
        
        # Perform cyclical and trend analysis
        self.cyclical_trend_analysis()
        
        # Create visualizations
        self.create_visualizations()
        
        # Create cyclical visualizations
        self.create_cyclical_visualizations()
        
        # Generate comprehensive report
        self.generate_comprehensive_report()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ Analysis completed successfully!")
        print(f"ğŸ“ Results saved in: {self.results_dir}")
        print(f"ğŸ“Š Plots available in: {self.plots_dir}")
        print(f"ğŸ“‹ Reports available in: {self.reports_dir}")
        
        return True

def main():
    """Main function to run the analysis"""
    analyzer = LoadDataAnalyzer()
    success = analyzer.run_complete_analysis()
    
    if success:
        print("\nâœ¨ Load data analysis completed successfully!")
        print("Check the 'analysis_results' folder for all outputs.")
    else:
        print("\nâŒ Analysis failed. Please check the data path and try again.")

if __name__ == "__main__":
    main()