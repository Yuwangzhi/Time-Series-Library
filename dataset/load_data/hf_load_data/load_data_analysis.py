#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Load Data Analysis Tool
Comprehensive analysis of load_data dataset for time series forecasting
Created on: 2024-09-21
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import json
import os
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Statistical analysis
from scipy import stats

# Try to import optional packages with fallbacks
try:
    import seaborn as sns
    HAS_SEABORN = True
except ImportError:
    HAS_SEABORN = False
    print("‚ö†Ô∏è  Seaborn not available - some visualizations will be simplified")

try:
    from statsmodels.tsa.stattools import adfuller
    from statsmodels.tsa.seasonal import seasonal_decompose
    from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
    HAS_STATSMODELS = True
except ImportError:
    HAS_STATSMODELS = False
    print("‚ö†Ô∏è  Statsmodels not available - some time series analyses will be skipped")

try:
    from sklearn.preprocessing import StandardScaler
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    print("‚ö†Ô∏è  Scikit-learn not available - some analyses will be simplified")

# Set plotting style
plt.style.use('default')
if HAS_SEABORN:
    sns.set_palette("husl")

class LoadDataAnalyzer:
    def __init__(self, data_path=None):
        """
        Initialize the Load Data Analyzer
        
        Args:
            data_path (str): Path to the CSV file, if None uses default path
        """
        if data_path is None:
            # Use current directory as default
            self.data_path = "./hf_load_data_20210101-20250807_processed.csv"
        else:
            self.data_path = data_path
            
        # Create results directory in current folder
        self.results_dir = Path("./analysis_results")
        self.plots_dir = self.results_dir / "plots"
        self.reports_dir = self.results_dir / "reports"
        
        # Create directories
        for dir_path in [self.results_dir, self.plots_dir, self.reports_dir]:
            dir_path.mkdir(exist_ok=True)
            
        self.df = None
        self.analysis_results = {}
        
    def load_data(self):
        """Load and initial check of the dataset"""
        print("üîç Loading load_data dataset...")
        
        try:
            self.df = pd.read_csv(self.data_path)
            print(f"‚úÖ Data loaded successfully: {self.df.shape}")
            
            # Display basic info
            print(f"üìä Dataset shape: {self.df.shape}")
            print(f"üìã Columns: {list(self.df.columns)}")
            print(f"üìÖ Date range: {self.df.iloc[0, 0]} to {self.df.iloc[-1, 0]}")
            
            # Convert date column to datetime if exists
            if 'date' in self.df.columns:
                self.df['date'] = pd.to_datetime(self.df['date'])
                self.df.set_index('date', inplace=True)
            elif self.df.columns[0].lower() in ['date', 'time', 'timestamp']:
                self.df.iloc[:, 0] = pd.to_datetime(self.df.iloc[:, 0])
                self.df.set_index(self.df.columns[0], inplace=True)
                
            return True
            
        except Exception as e:
            print(f"‚ùå Error loading data: {e}")
            return False
    
    def basic_statistics(self):
        """Perform basic statistical analysis"""
        print("\nüìà Performing basic statistical analysis...")
        
        results = {}
        
        # Basic info
        results['data_shape'] = self.df.shape
        results['columns'] = list(self.df.columns)
        results['data_types'] = self.df.dtypes.to_dict()
        
        # Missing values
        missing_info = self.df.isnull().sum().to_dict()
        results['missing_values'] = missing_info
        results['missing_percentage'] = {col: (count/len(self.df))*100 for col, count in missing_info.items()}
        
        # Descriptive statistics for numerical columns
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        results['descriptive_stats'] = {}
        
        for col in numeric_cols:
            col_stats = {
                'count': int(self.df[col].count()),
                'mean': float(self.df[col].mean()),
                'std': float(self.df[col].std()),
                'min': float(self.df[col].min()),
                'q25': float(self.df[col].quantile(0.25)),
                'median': float(self.df[col].median()),
                'q75': float(self.df[col].quantile(0.75)),
                'max': float(self.df[col].max()),
                'skewness': float(stats.skew(self.df[col].dropna())),
                'kurtosis': float(stats.kurtosis(self.df[col].dropna())),
                'range': float(self.df[col].max() - self.df[col].min()),
                'iqr': float(self.df[col].quantile(0.75) - self.df[col].quantile(0.25))
            }
            results['descriptive_stats'][col] = col_stats
        
        # Save results
        with open(self.reports_dir / 'basic_statistics.json', 'w') as f:
            json.dump(results, f, indent=4, default=str)
            
        self.analysis_results['basic_statistics'] = results
        print("‚úÖ Basic statistics analysis completed!")
        
        return results
    
    def time_series_analysis(self):
        """Analyze time series properties"""
        print("\n‚è∞ Performing time series analysis...")
        
        results = {}
        
        # Assume the main value column is 'value' or the last numeric column
        value_col = 'value' if 'value' in self.df.columns else self.df.select_dtypes(include=[np.number]).columns[-1]
        ts_data = self.df[value_col].dropna()
        
        # Frequency analysis
        if isinstance(self.df.index, pd.DatetimeIndex):
            results['frequency'] = str(pd.infer_freq(self.df.index))
            results['date_range'] = {
                'start': str(self.df.index.min()),
                'end': str(self.df.index.max()),
                'duration_days': (self.df.index.max() - self.df.index.min()).days
            }
        
        # Trend analysis (linear regression)
        x = np.arange(len(ts_data))
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, ts_data)
        results['trend_analysis'] = {
            'slope': float(slope),
            'intercept': float(intercept),
            'r_squared': float(r_value**2),
            'p_value': float(p_value),
            'trend_direction': 'increasing' if slope > 0 else 'decreasing'
        }
        
        # Stationarity test (ADF test)
        if HAS_STATSMODELS:
            try:
                adf_result = adfuller(ts_data)
                results['stationarity_adf'] = {
                    'adf_statistic': float(adf_result[0]),
                    'p_value': float(adf_result[1]),
                    'critical_values': {k: float(v) for k, v in adf_result[4].items()},
                    'is_stationary': adf_result[1] < 0.05
                }
            except Exception as e:
                results['stationarity_adf'] = {'error': str(e)}
        else:
            results['stationarity_adf'] = {'note': 'Statsmodels not available for ADF test'}
        
        # Autocorrelation analysis
        autocorr_lags = min(50, len(ts_data)//4)
        autocorr = [ts_data.autocorr(lag=i) for i in range(1, autocorr_lags+1)]
        results['autocorrelation'] = {
            'lags_1_to_10': [float(x) for x in autocorr[:10]],
            'significant_autocorr_count': sum(1 for x in autocorr if abs(x) > 0.1)
        }
        
        # Seasonality detection
        if len(ts_data) > 24:  # Need sufficient data for seasonality
            try:
                # Test for different seasonal periods
                seasonal_periods = [24, 168, 8760]  # Daily, weekly, yearly (for hourly data)
                results['seasonality'] = {}
                
                for period in seasonal_periods:
                    if len(ts_data) > 2 * period:
                        # Simple seasonal correlation test
                        seasonal_corr = np.corrcoef(ts_data[:-period], ts_data[period:])[0, 1]
                        results['seasonality'][f'period_{period}'] = float(seasonal_corr)
                        
            except Exception as e:
                results['seasonality'] = {'error': str(e)}
        
        # Save results
        with open(self.reports_dir / 'time_series_analysis.json', 'w') as f:
            json.dump(results, f, indent=4, default=str)
            
        self.analysis_results['time_series_analysis'] = results
        print("‚úÖ Time series analysis completed!")
        
        return results
    
    def data_quality_assessment(self):
        """Assess data quality"""
        print("\nüîç Performing data quality assessment...")
        
        results = {}
        
        # Missing value patterns
        results['missing_patterns'] = self.df.isnull().sum().to_dict()
        
        # Data consistency checks
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        results['consistency_checks'] = {}
        
        for col in numeric_cols:
            col_data = self.df[col].dropna()
            results['consistency_checks'][col] = {
                'negative_values_count': int((col_data < 0).sum()),
                'zero_values_count': int((col_data == 0).sum()),
                'infinite_values_count': int(np.isinf(col_data).sum()),
                'outliers_iqr_count': self._count_outliers_iqr(col_data),
                'outliers_zscore_count': self._count_outliers_zscore(col_data)
            }
        
        # Date gaps (if datetime index)
        if isinstance(self.df.index, pd.DatetimeIndex):
            expected_freq = pd.infer_freq(self.df.index)
            if expected_freq:
                full_range = pd.date_range(start=self.df.index.min(), 
                                         end=self.df.index.max(), 
                                         freq=expected_freq)
                missing_dates = len(full_range) - len(self.df.index)
                results['date_gaps'] = {
                    'expected_records': len(full_range),
                    'actual_records': len(self.df.index),
                    'missing_records': missing_dates,
                    'completeness_percentage': (len(self.df.index) / len(full_range)) * 100
                }
        
        # Overall quality score
        quality_score = self._calculate_quality_score()
        results['overall_quality_score'] = quality_score
        
        # Save results
        with open(self.reports_dir / 'data_quality_assessment.json', 'w') as f:
            json.dump(results, f, indent=4, default=str)
            
        self.analysis_results['data_quality_assessment'] = results
        print("‚úÖ Data quality assessment completed!")
        
        return results
    
    def _count_outliers_iqr(self, data):
        """Count outliers using IQR method"""
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        return int(((data < lower_bound) | (data > upper_bound)).sum())
    
    def _count_outliers_zscore(self, data, threshold=3):
        """Count outliers using Z-score method"""
        z_scores = np.abs(stats.zscore(data))
        return int((z_scores > threshold).sum())
    
    def _calculate_quality_score(self):
        """Calculate overall data quality score (0-100)"""
        score = 100
        
        # Penalize for missing values
        missing_ratio = self.df.isnull().sum().sum() / (self.df.shape[0] * self.df.shape[1])
        score -= missing_ratio * 30
        
        # Penalize for inconsistencies
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            col_data = self.df[col].dropna()
            if len(col_data) > 0:
                outlier_ratio = self._count_outliers_iqr(col_data) / len(col_data)
                score -= outlier_ratio * 10
        
        return max(0, score)
    
    def pattern_analysis(self):
        """Analyze patterns in the data"""
        print("\nüîÑ Performing pattern analysis...")
        
        results = {}
        
        # Main value column
        value_col = 'value' if 'value' in self.df.columns else self.df.select_dtypes(include=[np.number]).columns[-1]
        ts_data = self.df[value_col].dropna()
        
        # Periodic patterns
        if isinstance(self.df.index, pd.DatetimeIndex):
            df_with_time = self.df.copy()
            df_with_time['hour'] = df_with_time.index.hour
            df_with_time['day_of_week'] = df_with_time.index.dayofweek
            df_with_time['month'] = df_with_time.index.month
            
            results['periodic_patterns'] = {}
            
            # Hourly patterns
            if 'hour' in df_with_time.columns:
                hourly_stats = df_with_time.groupby('hour')[value_col].agg(['mean', 'std']).to_dict()
                results['periodic_patterns']['hourly'] = {
                    'mean_by_hour': {str(k): float(v) for k, v in hourly_stats['mean'].items()},
                    'std_by_hour': {str(k): float(v) for k, v in hourly_stats['std'].items()},
                    'peak_hour': int(df_with_time.groupby('hour')[value_col].mean().idxmax()),
                    'low_hour': int(df_with_time.groupby('hour')[value_col].mean().idxmin())
                }
            
            # Weekly patterns
            weekly_stats = df_with_time.groupby('day_of_week')[value_col].agg(['mean', 'std']).to_dict()
            results['periodic_patterns']['weekly'] = {
                'mean_by_day': {str(k): float(v) for k, v in weekly_stats['mean'].items()},
                'std_by_day': {str(k): float(v) for k, v in weekly_stats['std'].items()},
                'peak_day': int(df_with_time.groupby('day_of_week')[value_col].mean().idxmax()),
                'low_day': int(df_with_time.groupby('day_of_week')[value_col].mean().idxmin())
            }
            
            # Monthly patterns
            monthly_stats = df_with_time.groupby('month')[value_col].agg(['mean', 'std']).to_dict()
            results['periodic_patterns']['monthly'] = {
                'mean_by_month': {str(k): float(v) for k, v in monthly_stats['mean'].items()},
                'std_by_month': {str(k): float(v) for k, v in monthly_stats['std'].items()},
                'peak_month': int(df_with_time.groupby('month')[value_col].mean().idxmax()),
                'low_month': int(df_with_time.groupby('month')[value_col].mean().idxmin())
            }
        
        # Trend changes (change point detection - simple)
        window_size = min(100, len(ts_data)//10)
        if window_size > 10:
            rolling_mean = ts_data.rolling(window=window_size).mean()
            rolling_std = ts_data.rolling(window=window_size).std()
            
            results['trend_changes'] = {
                'high_volatility_periods': int((rolling_std > rolling_std.quantile(0.9)).sum()),
                'trend_reversals': self._detect_trend_reversals(rolling_mean),
                'volatility_range': {
                    'min_std': float(rolling_std.min()),
                    'max_std': float(rolling_std.max()),
                    'mean_std': float(rolling_std.mean())
                }
            }
        
        # Extreme values analysis
        results['extreme_values'] = {
            'top_5_values': ts_data.nlargest(5).tolist(),
            'bottom_5_values': ts_data.nsmallest(5).tolist(),
            'extreme_dates': {
                'max_date': str(ts_data.idxmax()),
                'min_date': str(ts_data.idxmin())
            }
        }
        
        # Save results
        with open(self.reports_dir / 'pattern_analysis.json', 'w') as f:
            json.dump(results, f, indent=4, default=str)
            
        self.analysis_results['pattern_analysis'] = results
        print("‚úÖ Pattern analysis completed!")
        
        return results
    
    def _detect_trend_reversals(self, data):
        """Detect trend reversals in the data"""
        if len(data) < 3:
            return 0
            
        diffs = np.diff(data.dropna())
        if len(diffs) < 2:
            return 0
            
        # Count sign changes in the differences
        sign_changes = np.sum(np.diff(np.sign(diffs)) != 0)
        return int(sign_changes)
    
    def cyclical_trend_analysis(self):
        """Ê∑±Â∫¶ÂàÜÊûêÁü≠Âë®ÊúüÂíåÈïøÂë®ÊúüÁöÑË∂ãÂäøÂèòÂåñ"""
        print("\nüîÑ Performing cyclical and trend analysis...")
        
        results = {}
        value_col = 'value' if 'value' in self.df.columns else self.df.select_dtypes(include=[np.number]).columns[-1]
        
        if not isinstance(self.df.index, pd.DatetimeIndex):
            print("‚ö†Ô∏è  ÈúÄË¶ÅÊó∂Èó¥Á¥¢ÂºïËøõË°åÂë®ÊúüÊÄßÂàÜÊûê")
            return results
        
        # Áü≠Âë®ÊúüÂàÜÊûê (Êó•„ÄÅÂë®)
        short_cycle_results = self._analyze_short_cycles(value_col)
        results['short_cycles'] = short_cycle_results
        
        # ÈïøÂë®ÊúüÂàÜÊûê (Êúà„ÄÅÂ≠£„ÄÅÂπ¥)
        long_cycle_results = self._analyze_long_cycles(value_col)
        results['long_cycles'] = long_cycle_results
        
        # ÂÖ∏ÂûãÊ°à‰æãÂàÜÊûê
        case_study_results = self._generate_case_studies(value_col)
        results['case_studies'] = case_study_results
        
        # Save results
        with open(self.reports_dir / 'cyclical_trend_analysis.json', 'w') as f:
            json.dump(results, f, indent=4, default=str)
            
        self.analysis_results['cyclical_trend_analysis'] = results
        print("‚úÖ Cyclical and trend analysis completed!")
        
        return results
    
    def _analyze_short_cycles(self, value_col):
        """ÂàÜÊûêÁü≠Âë®ÊúüÊ®°Âºè (Â∞èÊó∂„ÄÅÊó•„ÄÅÂë®)"""
        results = {}
        
        # ÊØèÊó•Ê®°ÂºèÂàÜÊûê
        daily_pattern = self._analyze_daily_pattern(value_col)
        results['daily_pattern'] = daily_pattern
        
        # ÊØèÂë®Ê®°ÂºèÂàÜÊûê
        weekly_pattern = self._analyze_weekly_pattern(value_col)
        results['weekly_pattern'] = weekly_pattern
        
        # Â∑•‰ΩúÊó•vsÂë®Êú´ÂØπÊØî
        weekday_weekend = self._analyze_weekday_weekend(value_col)
        results['weekday_weekend'] = weekday_weekend
        
        return results
    
    def _analyze_long_cycles(self, value_col):
        """ÂàÜÊûêÈïøÂë®ÊúüÊ®°Âºè (Êúà„ÄÅÂ≠£„ÄÅÂπ¥)"""
        results = {}
        
        # ÊúàÂ∫¶Ê®°ÂºèÂàÜÊûê
        monthly_pattern = self._analyze_monthly_pattern(value_col)
        results['monthly_pattern'] = monthly_pattern
        
        # Â≠£ËäÇÊÄßÊ®°ÂºèÂàÜÊûê
        seasonal_pattern = self._analyze_seasonal_pattern(value_col)
        results['seasonal_pattern'] = seasonal_pattern
        
        # Âπ¥Â∫¶Ë∂ãÂäøÂàÜÊûê
        yearly_trend = self._analyze_yearly_trend(value_col)
        results['yearly_trend'] = yearly_trend
        
        return results
    
    def _analyze_daily_pattern(self, value_col):
        """ÂàÜÊûêÊó•ÂÜÖË¥üËç∑ÂèòÂåñÊ®°Âºè"""
        df_hourly = self.df.copy()
        df_hourly['hour'] = df_hourly.index.hour
        
        hourly_stats = df_hourly.groupby('hour')[value_col].agg(['mean', 'std', 'min', 'max']).round(2)
        
        # ËÆ°ÁÆóÂ≥∞Ë∞∑Â∑ÆÂíåÂèòÂåñÁéá
        daily_min = hourly_stats['mean'].min()
        daily_max = hourly_stats['mean'].max()
        peak_valley_ratio = (daily_max - daily_min) / daily_min * 100
        
        # ËØÜÂà´Ë¥üËç∑ÁâπÂæÅÊó∂ÊÆµ
        peak_hour = hourly_stats['mean'].idxmax()
        valley_hour = hourly_stats['mean'].idxmin()
        
        return {
            'hourly_statistics': hourly_stats.to_dict(),
            'peak_hour': int(peak_hour),
            'valley_hour': int(valley_hour),
            'daily_min_load': float(daily_min),
            'daily_max_load': float(daily_max),
            'peak_valley_ratio': float(peak_valley_ratio),
            'load_factor': float(hourly_stats['mean'].mean() / daily_max * 100)
        }
    
    def _analyze_weekly_pattern(self, value_col):
        """ÂàÜÊûêÂë®ÂÜÖË¥üËç∑ÂèòÂåñÊ®°Âºè"""
        df_weekly = self.df.copy()
        df_weekly['day_of_week'] = df_weekly.index.dayofweek
        df_weekly['day_name'] = df_weekly.index.day_name()
        
        weekly_stats = df_weekly.groupby(['day_of_week', 'day_name'])[value_col].agg(['mean', 'std']).round(2)
        
        # ËÆ°ÁÆóÂ∑•‰ΩúÊó•ÂíåÂë®Êú´ÁöÑÂπ≥ÂùáË¥üËç∑
        workday_load = df_weekly[df_weekly['day_of_week'] < 5][value_col].mean()
        weekend_load = df_weekly[df_weekly['day_of_week'] >= 5][value_col].mean()
        
        # ËΩ¨Êç¢‰∏∫JSONÂèØÂ∫èÂàóÂåñÁöÑÊ†ºÂºè
        weekly_stats_dict = {}
        for (day_num, day_name), row in weekly_stats.iterrows():
            key = f"{day_num}_{day_name}"
            weekly_stats_dict[key] = {
                'mean': float(row['mean']),
                'std': float(row['std'])
            }
        
        peak_idx = weekly_stats['mean'].idxmax()
        low_idx = weekly_stats['mean'].idxmin()
        
        return {
            'weekly_statistics': weekly_stats_dict,
            'workday_average': float(workday_load),
            'weekend_average': float(weekend_load),
            'workday_weekend_ratio': float(workday_load / weekend_load),
            'peak_day': peak_idx[1] if isinstance(peak_idx, tuple) else str(peak_idx),
            'low_day': low_idx[1] if isinstance(low_idx, tuple) else str(low_idx)
        }
    
    def _analyze_weekday_weekend(self, value_col):
        """ÂàÜÊûêÂ∑•‰ΩúÊó•‰∏éÂë®Êú´ÁöÑË¥üËç∑Â∑ÆÂºÇ"""
        df_compare = self.df.copy()
        df_compare['is_weekend'] = df_compare.index.dayofweek >= 5
        df_compare['hour'] = df_compare.index.hour
        
        # ÊåâÂ∞èÊó∂ÊØîËæÉÂ∑•‰ΩúÊó•ÂíåÂë®Êú´
        comparison = df_compare.groupby(['hour', 'is_weekend'])[value_col].mean().unstack()
        
        # ËΩ¨Êç¢‰∏∫JSONÂèØÂ∫èÂàóÂåñÁöÑÊ†ºÂºè
        comparison_dict = {}
        if not comparison.empty:
            for hour in comparison.index:
                hour_data = {}
                if False in comparison.columns:
                    hour_data['weekday'] = float(comparison.loc[hour, False])
                if True in comparison.columns:
                    hour_data['weekend'] = float(comparison.loc[hour, True])
                comparison_dict[str(hour)] = hour_data
        
        # ËÆ°ÁÆóÂ∑ÆÂºÇÁªüËÆ°
        if False in comparison.columns and True in comparison.columns:
            hourly_diff = comparison[False] - comparison[True]  # Â∑•‰ΩúÊó• - Âë®Êú´
            max_diff_hour = hourly_diff.idxmax()
            max_diff_value = hourly_diff.max()
            avg_diff = hourly_diff.mean()
        else:
            max_diff_hour = 0
            max_diff_value = 0
            avg_diff = 0
        
        return {
            'hourly_comparison': comparison_dict,
            'max_difference_hour': int(max_diff_hour),
            'max_difference_value': float(max_diff_value),
            'average_difference': float(avg_diff)
        }
    
    def _analyze_monthly_pattern(self, value_col):
        """ÂàÜÊûêÊúàÂ∫¶Ë¥üËç∑ÂèòÂåñÊ®°Âºè"""
        df_monthly = self.df.copy()
        df_monthly['month'] = df_monthly.index.month
        df_monthly['month_name'] = df_monthly.index.month_name()
        
        monthly_stats = df_monthly.groupby(['month', 'month_name'])[value_col].agg(['mean', 'std', 'min', 'max']).round(2)
        
        # ËÆ°ÁÆóÂ≠£ËäÇÊÄßÊåáÊ†á
        spring_load = df_monthly[df_monthly['month'].isin([3, 4, 5])][value_col].mean()
        summer_load = df_monthly[df_monthly['month'].isin([6, 7, 8])][value_col].mean()
        autumn_load = df_monthly[df_monthly['month'].isin([9, 10, 11])][value_col].mean()
        winter_load = df_monthly[df_monthly['month'].isin([12, 1, 2])][value_col].mean()
        
        # ËΩ¨Êç¢‰∏∫JSONÂèØÂ∫èÂàóÂåñÁöÑÊ†ºÂºè
        monthly_stats_dict = {}
        for (month_num, month_name), row in monthly_stats.iterrows():
            key = f"{month_num}_{month_name}"
            monthly_stats_dict[key] = {
                'mean': float(row['mean']),
                'std': float(row['std']),
                'min': float(row['min']),
                'max': float(row['max'])
            }
        
        peak_idx = monthly_stats['mean'].idxmax()
        low_idx = monthly_stats['mean'].idxmin()
        
        return {
            'monthly_statistics': monthly_stats_dict,
            'seasonal_averages': {
                'spring': float(spring_load),
                'summer': float(summer_load),
                'autumn': float(autumn_load),
                'winter': float(winter_load)
            },
            'peak_month': peak_idx[1] if isinstance(peak_idx, tuple) else str(peak_idx),
            'low_month': low_idx[1] if isinstance(low_idx, tuple) else str(low_idx)
        }
    
    def _analyze_seasonal_pattern(self, value_col):
        """ÂàÜÊûêÂ≠£ËäÇÊÄßË¥üËç∑ÁâπÂæÅ"""
        df_seasonal = self.df.copy()
        df_seasonal['month'] = df_seasonal.index.month
        df_seasonal['season'] = pd.Series(df_seasonal['month']).apply(self._get_season).values
        
        seasonal_stats = df_seasonal.groupby('season')[value_col].agg(['mean', 'std', 'min', 'max']).round(2)
        
        # ËÆ°ÁÆóÂ≠£ËäÇÊÄßÂèòÂåñÁ≥ªÊï∞
        seasonal_cv = (seasonal_stats['std'] / seasonal_stats['mean'] * 100).round(2)
        
        return {
            'seasonal_statistics': seasonal_stats.to_dict(),
            'seasonal_cv': seasonal_cv.to_dict(),
            'highest_season': seasonal_stats['mean'].idxmax(),
            'lowest_season': seasonal_stats['mean'].idxmin()
        }
    
    def _analyze_yearly_trend(self, value_col):
        """ÂàÜÊûêÂπ¥Â∫¶Ë∂ãÂäøÂèòÂåñ"""
        df_yearly = self.df.copy()
        df_yearly['year'] = df_yearly.index.year
        
        yearly_stats = df_yearly.groupby('year')[value_col].agg(['mean', 'std', 'min', 'max']).round(2)
        
        # ËÆ°ÁÆóÂπ¥Â∫¶Â¢ûÈïøÁéá
        if len(yearly_stats) > 1:
            growth_rates = yearly_stats['mean'].pct_change().dropna() * 100
            avg_growth_rate = growth_rates.mean()
        else:
            growth_rates = pd.Series()
            avg_growth_rate = 0
        
        return {
            'yearly_statistics': yearly_stats.to_dict(),
            'growth_rates': growth_rates.to_dict(),
            'average_growth_rate': float(avg_growth_rate),
            'trend_direction': 'increasing' if avg_growth_rate > 0 else 'decreasing'
        }
    
    def _get_season(self, month):
        """Ê†πÊçÆÊúà‰ªΩÂà§Êñ≠Â≠£ËäÇ"""
        if month in [3, 4, 5]:
            return 'Spring'
        elif month in [6, 7, 8]:
            return 'Summer'
        elif month in [9, 10, 11]:
            return 'Autumn'
        else:
            return 'Winter'
    
    def _generate_case_studies(self, value_col):
        """ÁîüÊàêÂÖ∏ÂûãÊ°à‰æãÂàÜÊûê"""
        results = {}
        
        # ÈÄâÊã©‰ª£Ë°®ÊÄßÁöÑÊó∂Èó¥ÊÆµËøõË°åÊ°à‰æãÂàÜÊûê
        case_periods = {
            'typical_day': self._get_typical_day(value_col),
            'peak_week': self._get_peak_week(value_col),
            'seasonal_comparison': self._get_seasonal_comparison(value_col)
        }
        
        for case_name, case_data in case_periods.items():
            if case_data is not None:
                results[case_name] = case_data
        
        return results
    
    def _get_typical_day(self, value_col):
        """Ëé∑ÂèñÂÖ∏ÂûãÊó•Ë¥üËç∑Êõ≤Á∫ø"""
        # ÈÄâÊã©‰∏Ä‰∏™Â∑•‰ΩúÊó•‰Ωú‰∏∫ÂÖ∏ÂûãÊó•
        workdays = self.df[self.df.index.dayofweek < 5]
        if workdays.empty:
            return None
        
        # ËÆ°ÁÆóÂπ≥ÂùáÊó•Ë¥üËç∑Êõ≤Á∫ø
        typical_day = workdays.groupby(workdays.index.hour)[value_col].mean()
        
        # ÈÄâÊã©ÊúÄÊé•ËøëÂπ≥ÂùáÂÄºÁöÑÂÆûÈôÖÊó•Êúü
        daily_avg = workdays.groupby(workdays.index.date)[value_col].mean()
        overall_avg = daily_avg.mean()
        closest_date = daily_avg.sub(overall_avg).abs().idxmin()
        
        actual_day = workdays[workdays.index.date == closest_date]
        
        # ËΩ¨Êç¢‰∏∫JSONÂèØÂ∫èÂàóÂåñÁöÑÊ†ºÂºè
        typical_curve_dict = {str(hour): float(load) for hour, load in typical_day.items()}
        actual_curve_dict = {str(idx): float(val) for idx, val in actual_day[value_col].items()}
        
        return {
            'date': str(closest_date),
            'typical_curve': typical_curve_dict,
            'actual_curve': actual_curve_dict,
            'daily_statistics': {
                'mean': float(actual_day[value_col].mean()),
                'min': float(actual_day[value_col].min()),
                'max': float(actual_day[value_col].max()),
                'std': float(actual_day[value_col].std())
            }
        }
    
    def _get_peak_week(self, value_col):
        """Ëé∑ÂèñÂ≥∞ÂÄºÂë®ÁöÑË¥üËç∑ÁâπÂæÅ"""
        # ÊåâÂë®ËÆ°ÁÆóÂπ≥ÂùáË¥üËç∑ÔºåÊâæÂà∞Â≥∞ÂÄºÂë®
        weekly_avg = self.df.groupby(pd.Grouper(freq='W'))[value_col].mean()
        if weekly_avg.empty:
            return None
        
        peak_week_start = weekly_avg.idxmax()
        peak_week_end = peak_week_start + pd.Timedelta(days=6)
        
        peak_week_data = self.df[peak_week_start:peak_week_end]
        
        # ËΩ¨Êç¢‰∏∫JSONÂèØÂ∫èÂàóÂåñÁöÑÊ†ºÂºè
        weekly_pattern = peak_week_data.groupby(peak_week_data.index.dayofweek)[value_col].mean()
        weekly_pattern_dict = {str(day): float(load) for day, load in weekly_pattern.items()}
        
        return {
            'week_start': str(peak_week_start.date()),
            'week_end': str(peak_week_end.date()),
            'weekly_pattern': weekly_pattern_dict,
            'week_statistics': {
                'mean': float(peak_week_data[value_col].mean()),
                'min': float(peak_week_data[value_col].min()),
                'max': float(peak_week_data[value_col].max()),
                'std': float(peak_week_data[value_col].std())
            }
        }
    
    def _get_seasonal_comparison(self, value_col):
        """Ëé∑ÂèñÂ≠£ËäÇÊÄßÂØπÊØîÂàÜÊûê"""
        seasons = {}
        for season_name, months in [('Summer', [6, 7, 8]), ('Winter', [12, 1, 2])]:
            season_data = self.df[self.df.index.month.isin(months)]
            if not season_data.empty:
                daily_pattern = season_data.groupby(season_data.index.hour)[value_col].mean()
                daily_pattern_dict = {str(hour): float(load) for hour, load in daily_pattern.items()}
                
                seasons[season_name] = {
                    'average_load': float(season_data[value_col].mean()),
                    'daily_pattern': daily_pattern_dict,
                    'statistics': {
                        'mean': float(season_data[value_col].mean()),
                        'std': float(season_data[value_col].std()),
                        'min': float(season_data[value_col].min()),
                        'max': float(season_data[value_col].max())
                    }
                }
        
        # ËÆ°ÁÆóÂ≠£ËäÇÊÄßÂ∑ÆÂºÇ
        if 'Summer' in seasons and 'Winter' in seasons:
            seasonal_difference = seasons['Summer']['average_load'] - seasons['Winter']['average_load']
            seasons['comparison'] = {
                'difference': float(seasonal_difference),
                'ratio': float(seasons['Summer']['average_load'] / seasons['Winter']['average_load'])
            }
        
        return seasons
    
    def create_cyclical_visualizations(self):
        """ÂàõÂª∫Âë®ÊúüÊÄßÂíåË∂ãÂäøÊÄßÂèØËßÜÂåñÂõæË°®"""
        print("\nüìä Creating cyclical and trend visualizations...")
        
        value_col = 'value' if 'value' in self.df.columns else self.df.select_dtypes(include=[np.number]).columns[-1]
        
        # Áü≠Âë®ÊúüÂèØËßÜÂåñ
        self._plot_short_cycle_analysis(value_col)
        
        # ÈïøÂë®ÊúüÂèØËßÜÂåñ
        self._plot_long_cycle_analysis(value_col)
        
        # ÂÖ∏ÂûãÊ°à‰æãÂèØËßÜÂåñ
        self._plot_case_studies(value_col)
        
        print("‚úÖ Cyclical and trend visualizations created!")
    
    def _plot_short_cycle_analysis(self, value_col):
        """ÁªòÂà∂Áü≠Âë®ÊúüÂàÜÊûêÂõæË°®"""
        # Set font to support Chinese characters but use English labels
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. Êó•ÂÜÖË¥üËç∑Êõ≤Á∫ø
        hourly_data = self.df.groupby(self.df.index.hour)[value_col].agg(['mean', 'std'])
        axes[0, 0].plot(hourly_data.index, hourly_data['mean'], 'b-', linewidth=2, label='Average Load')
        axes[0, 0].fill_between(hourly_data.index, 
                               hourly_data['mean'] - hourly_data['std'],
                               hourly_data['mean'] + hourly_data['std'],
                               alpha=0.3, label='¬±1 Std Dev')
        axes[0, 0].set_title('Daily Load Pattern', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('Hour')
        axes[0, 0].set_ylabel('Load (MW)')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. Âë®ÂÜÖË¥üËç∑Ê®°Âºè
        weekly_data = self.df.groupby(self.df.index.dayofweek)[value_col].mean()
        day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
        axes[0, 1].bar(range(7), weekly_data.values, alpha=0.7, color='skyblue')
        axes[0, 1].set_title('Weekly Load Distribution', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('Day of Week')
        axes[0, 1].set_ylabel('Average Load (MW)')
        axes[0, 1].set_xticks(range(7))
        axes[0, 1].set_xticklabels(day_names, rotation=45)
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. Â∑•‰ΩúÊó•vsÂë®Êú´ÂØπÊØî
        df_compare = self.df.copy()
        df_compare['is_weekend'] = df_compare.index.dayofweek >= 5
        df_compare['hour'] = df_compare.index.hour
        comparison = df_compare.groupby(['hour', 'is_weekend'])[value_col].mean().unstack()
        
        if False in comparison.columns and True in comparison.columns:
            axes[1, 0].plot(comparison.index, comparison[False], 'b-', linewidth=2, label='Weekday')
            axes[1, 0].plot(comparison.index, comparison[True], 'r-', linewidth=2, label='Weekend')
        axes[1, 0].set_title('Weekday vs Weekend Load Comparison', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('Hour')
        axes[1, 0].set_ylabel('Average Load (MW)')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. Êó•Ë¥üËç∑Á≥ªÊï∞ÂàÜÂ∏É
        daily_load_factor = self.df.groupby(self.df.index.date)[value_col].agg(['mean', 'max'])
        daily_load_factor['load_factor'] = daily_load_factor['mean'] / daily_load_factor['max']
        axes[1, 1].hist(daily_load_factor['load_factor'], bins=30, alpha=0.7, edgecolor='black')
        axes[1, 1].axvline(daily_load_factor['load_factor'].mean(), color='red', linestyle='--', 
                          label=f'Mean: {daily_load_factor["load_factor"].mean():.3f}')
        axes[1, 1].set_title('Daily Load Factor Distribution', fontsize=14, fontweight='bold')
        axes[1, 1].set_xlabel('Load Factor')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'short_cycle_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_long_cycle_analysis(self, value_col):
        """ÁªòÂà∂ÈïøÂë®ÊúüÂàÜÊûêÂõæË°®"""
        # Set font to support Chinese characters but use English labels
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. ÊúàÂ∫¶Ë¥üËç∑ÂèòÂåñ
        monthly_data = self.df.groupby(self.df.index.month)[value_col].agg(['mean', 'std'])
        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
                      'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
        axes[0, 0].plot(monthly_data.index, monthly_data['mean'], 'o-', linewidth=2, markersize=6)
        axes[0, 0].errorbar(monthly_data.index, monthly_data['mean'], yerr=monthly_data['std'], 
                           capsize=5, alpha=0.7)
        axes[0, 0].set_title('Monthly Load Trend', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('Month')
        axes[0, 0].set_ylabel('Average Load (MW)')
        axes[0, 0].set_xticks(range(1, 13))
        axes[0, 0].set_xticklabels(month_names, rotation=45)
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. Â≠£ËäÇÊÄßË¥üËç∑ÂàÜÂ∏É
        seasonal_data = self.df.copy()
        seasonal_data['season'] = pd.Series(seasonal_data.index.month).apply(self._get_season).values
        season_avg = seasonal_data.groupby('season')[value_col].mean()
        colors = ['lightgreen', 'gold', 'orange', 'lightblue']
        axes[0, 1].bar(season_avg.index, season_avg.values, color=colors, alpha=0.7)
        axes[0, 1].set_title('Seasonal Load Distribution', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('Season')
        axes[0, 1].set_ylabel('Average Load (MW)')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. Âπ¥Â∫¶Ë∂ãÂäø
        if len(self.df.index.year.unique()) > 1:
            yearly_data = self.df.groupby(self.df.index.year)[value_col].mean()
            axes[1, 0].plot(yearly_data.index, yearly_data.values, 'o-', linewidth=2, markersize=8)
            
            # Ê∑ªÂä†Ë∂ãÂäøÁ∫ø
            x = np.arange(len(yearly_data))
            coeffs = np.polyfit(x, yearly_data.values, 1)
            trend_line = coeffs[0] * x + coeffs[1]
            axes[1, 0].plot(yearly_data.index, trend_line, '--', color='red', alpha=0.7, 
                           label=f'Trend: {coeffs[0]:.2f} MW/year')
            axes[1, 0].legend()
        else:
            axes[1, 0].text(0.5, 0.5, 'Insufficient data\nfor yearly trend analysis', 
                           ha='center', va='center', transform=axes[1, 0].transAxes, fontsize=12)
        
        axes[1, 0].set_title('Annual Load Trend', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('Year')
        axes[1, 0].set_ylabel('Annual Average Load (MW)')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. ÈïøÊúüÂèòÂåñÁ≥ªÊï∞
        monthly_cv = self.df.groupby([self.df.index.year, self.df.index.month])[value_col].std() / \
                    self.df.groupby([self.df.index.year, self.df.index.month])[value_col].mean()
        if not monthly_cv.empty:
            axes[1, 1].plot(range(len(monthly_cv)), monthly_cv.values, linewidth=1.5)
            axes[1, 1].axhline(monthly_cv.mean(), color='red', linestyle='--', 
                              label=f'Mean CV: {monthly_cv.mean():.3f}')
            axes[1, 1].legend()
        axes[1, 1].set_title('Load Coefficient of Variation Over Time', fontsize=14, fontweight='bold')
        axes[1, 1].set_xlabel('Time Series')
        axes[1, 1].set_ylabel('Coefficient of Variation')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'long_cycle_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_case_studies(self, value_col):
        """ÁªòÂà∂ÂÖ∏ÂûãÊ°à‰æãÂàÜÊûêÂõæË°®"""
        # Set font to support Chinese characters but use English labels
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. ÂÖ∏ÂûãÊó•Ë¥üËç∑Êõ≤Á∫ø
        workdays = self.df[self.df.index.dayofweek < 5]
        if not workdays.empty:
            # ÈÄâÊã©‰∏Ä‰∏™‰ª£Ë°®ÊÄßÂ∑•‰ΩúÊó•
            daily_avg = workdays.groupby(workdays.index.date)[value_col].mean()
            overall_avg = daily_avg.mean()
            closest_date = daily_avg.sub(overall_avg).abs().idxmin()
            typical_day = workdays[workdays.index.date == closest_date]
            
            # ÁªòÂà∂ÂÖ∏ÂûãÊó•ÂíåÂπ≥ÂùáÊó•Êõ≤Á∫ø
            avg_hourly = workdays.groupby(workdays.index.hour)[value_col].mean()
            axes[0, 0].plot(typical_day.index.hour, typical_day[value_col], 'b-', 
                           linewidth=2, label=f'Typical Day ({closest_date})')
            axes[0, 0].plot(avg_hourly.index, avg_hourly.values, 'r--', 
                           linewidth=2, label='Weekday Average')
            
        axes[0, 0].set_title('Typical Weekday Load Curve', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('Hour')
        axes[0, 0].set_ylabel('Load (MW)')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. Â≥∞ÂÄºÂë®ÂàÜÊûê
        weekly_avg = self.df.groupby(pd.Grouper(freq='W'))[value_col].mean()
        if not weekly_avg.empty:
            peak_week_start = weekly_avg.idxmax()
            peak_week_end = peak_week_start + pd.Timedelta(days=6)
            peak_week_data = self.df[peak_week_start:peak_week_end]
            
            axes[0, 1].plot(peak_week_data.index, peak_week_data[value_col], linewidth=1.5)
            axes[0, 1].set_title(f'Peak Week Load Pattern ({peak_week_start.date()} - {peak_week_end.date()})', 
                                fontsize=14, fontweight='bold')
            axes[0, 1].set_xlabel('Date')
            axes[0, 1].set_ylabel('Load (MW)')
            axes[0, 1].grid(True, alpha=0.3)
        
        # 3. Â≠£ËäÇÊÄßÂØπÊØî (Â§èÂ≠£vsÂÜ¨Â≠£)
        summer_data = self.df[self.df.index.month.isin([6, 7, 8])]
        winter_data = self.df[self.df.index.month.isin([12, 1, 2])]
        
        if not summer_data.empty and not winter_data.empty:
            summer_hourly = summer_data.groupby(summer_data.index.hour)[value_col].mean()
            winter_hourly = winter_data.groupby(winter_data.index.hour)[value_col].mean()
            
            axes[1, 0].plot(summer_hourly.index, summer_hourly.values, 'r-', 
                           linewidth=2, label='Summer Average')
            axes[1, 0].plot(winter_hourly.index, winter_hourly.values, 'b-', 
                           linewidth=2, label='Winter Average')
            
        axes[1, 0].set_title('Summer vs Winter Load Comparison', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('Hour')
        axes[1, 0].set_ylabel('Average Load (MW)')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. Ë¥üËç∑ÊåÅÁª≠Êõ≤Á∫ø
        load_duration = np.sort(self.df[value_col].values)[::-1]
        duration_percent = np.arange(1, len(load_duration) + 1) / len(load_duration) * 100
        
        axes[1, 1].plot(duration_percent, load_duration, linewidth=2)
        axes[1, 1].axhline(self.df[value_col].mean(), color='red', linestyle='--', 
                          label=f'Average Load: {self.df[value_col].mean():.1f} MW')
        axes[1, 1].axvline(50, color='orange', linestyle='--', alpha=0.7, label='Median')
        axes[1, 1].set_title('Load Duration Curve', fontsize=14, fontweight='bold')
        axes[1, 1].set_xlabel('Duration Percentage (%)')
        axes[1, 1].set_ylabel('Load (MW)')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'case_studies.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def create_visualizations(self):
        """Create comprehensive visualizations"""
        print("\nüìä Creating visualizations...")
        
        # Set font to support Chinese characters but use English labels
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False
        
        # Main value column
        value_col = 'value' if 'value' in self.df.columns else self.df.select_dtypes(include=[np.number]).columns[-1]
        
        # 1. Time series trend plot
        self._plot_time_series_trend(value_col)
        
        # 2. Distribution analysis
        self._plot_distribution_analysis(value_col)
        
        # 3. Seasonality analysis
        self._plot_seasonality_analysis(value_col)
        
        # 4. Correlation heatmap
        self._plot_correlation_heatmap()
        
        # 5. Rolling statistics
        self._plot_rolling_statistics(value_col)
        
        # 6. Outlier detection
        self._plot_outlier_detection(value_col)
        
        # 7. Autocorrelation
        self._plot_autocorrelation(value_col)
        
        # 8. Time series decomposition
        self._plot_decomposition(value_col)
        
        print("‚úÖ All visualizations created!")
    
    def _plot_time_series_trend(self, value_col):
        """Plot time series trend"""
        plt.figure(figsize=(15, 8))
        
        plt.subplot(2, 1, 1)
        plt.plot(self.df.index, self.df[value_col], alpha=0.7, linewidth=0.8)
        plt.title(f'Time Series Trend - {value_col}', fontsize=14, fontweight='bold')
        plt.ylabel('Value')
        plt.grid(True, alpha=0.3)
        
        plt.subplot(2, 1, 2)
        # Rolling statistics
        rolling_mean = self.df[value_col].rolling(window=24*7).mean()  # Weekly rolling mean
        rolling_std = self.df[value_col].rolling(window=24*7).std()    # Weekly rolling std
        
        plt.plot(self.df.index, self.df[value_col], alpha=0.3, label='Original', linewidth=0.5)
        plt.plot(self.df.index, rolling_mean, 'r-', label='Rolling Mean', linewidth=1.5)
        plt.fill_between(self.df.index, 
                        rolling_mean - rolling_std, 
                        rolling_mean + rolling_std, 
                        alpha=0.2, color='red', label='¬±1 Std')
        plt.title('Time Series with Rolling Statistics', fontsize=14, fontweight='bold')
        plt.ylabel('Value')
        plt.xlabel('Date')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'time_series_trend.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_distribution_analysis(self, value_col):
        """Plot distribution analysis"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        data = self.df[value_col].dropna()
        
        # Histogram
        axes[0, 0].hist(data, bins=50, alpha=0.7, density=True, edgecolor='black')
        axes[0, 0].axvline(data.mean(), color='red', linestyle='--', label=f'Mean: {data.mean():.2f}')
        axes[0, 0].axvline(data.median(), color='green', linestyle='--', label=f'Median: {data.median():.2f}')
        axes[0, 0].set_title('Histogram', fontweight='bold')
        axes[0, 0].set_xlabel('Value')
        axes[0, 0].set_ylabel('Density')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Box plot
        axes[0, 1].boxplot(data, vert=True)
        axes[0, 1].set_title('Box Plot', fontweight='bold')
        axes[0, 1].set_ylabel('Value')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Q-Q plot
        stats.probplot(data, dist="norm", plot=axes[1, 0])
        axes[1, 0].set_title('Q-Q Plot (Normal Distribution)', fontweight='bold')
        axes[1, 0].grid(True, alpha=0.3)
        
        # Cumulative distribution
        sorted_data = np.sort(data)
        cumulative = np.arange(1, len(sorted_data) + 1) / len(sorted_data)
        axes[1, 1].plot(sorted_data, cumulative, linewidth=2)
        axes[1, 1].set_title('Cumulative Distribution Function', fontweight='bold')
        axes[1, 1].set_xlabel('Value')
        axes[1, 1].set_ylabel('Cumulative Probability')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'distribution_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_seasonality_analysis(self, value_col):
        """Plot seasonality analysis"""
        if not isinstance(self.df.index, pd.DatetimeIndex):
            return
            
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        df_with_time = self.df.copy()
        df_with_time['hour'] = df_with_time.index.hour
        df_with_time['day_of_week'] = df_with_time.index.dayofweek
        df_with_time['month'] = df_with_time.index.month
        
        # Hourly pattern
        hourly_mean = df_with_time.groupby('hour')[value_col].mean()
        axes[0, 0].plot(hourly_mean.index, hourly_mean.values, marker='o', linewidth=2)
        axes[0, 0].set_title('Average by Hour of Day', fontweight='bold')
        axes[0, 0].set_xlabel('Hour')
        axes[0, 0].set_ylabel('Average Value')
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].set_xticks(range(0, 24, 2))
        
        # Daily pattern
        daily_mean = df_with_time.groupby('day_of_week')[value_col].mean()
        day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
        axes[0, 1].bar(range(7), daily_mean.values, alpha=0.7)
        axes[0, 1].set_title('Average by Day of Week', fontweight='bold')
        axes[0, 1].set_xlabel('Day of Week')
        axes[0, 1].set_ylabel('Average Value')
        axes[0, 1].set_xticks(range(7))
        axes[0, 1].set_xticklabels(day_names)
        axes[0, 1].grid(True, alpha=0.3)
        
        # Monthly pattern
        monthly_mean = df_with_time.groupby('month')[value_col].mean()
        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
                      'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
        axes[1, 0].plot(monthly_mean.index, monthly_mean.values, marker='s', linewidth=2)
        axes[1, 0].set_title('Average by Month', fontweight='bold')
        axes[1, 0].set_xlabel('Month')
        axes[1, 0].set_ylabel('Average Value')
        axes[1, 0].set_xticks(range(1, 13))
        axes[1, 0].set_xticklabels(month_names, rotation=45)
        axes[1, 0].grid(True, alpha=0.3)
        
        # Heatmap: Hour vs Day of Week
        heatmap_data = df_with_time.pivot_table(values=value_col, index='hour', columns='day_of_week', aggfunc='mean')
        im = axes[1, 1].imshow(heatmap_data.values, cmap='viridis', aspect='auto')
        axes[1, 1].set_title('Heatmap: Hour vs Day of Week', fontweight='bold')
        axes[1, 1].set_xlabel('Day of Week')
        axes[1, 1].set_ylabel('Hour')
        axes[1, 1].set_xticks(range(7))
        axes[1, 1].set_xticklabels(day_names)
        axes[1, 1].set_yticks(range(0, 24, 4))
        plt.colorbar(im, ax=axes[1, 1])
        
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'seasonality_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_correlation_heatmap(self):
        """Plot correlation heatmap"""
        numeric_cols = self.df.select_dtypes(include=[np.number])
        if len(numeric_cols.columns) < 2:
            return
            
        plt.figure(figsize=(10, 8))
        correlation_matrix = numeric_cols.corr()
        
        if HAS_SEABORN:
            mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
            sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', 
                       center=0, square=True, linewidths=0.5)
        else:
            # Fallback to matplotlib imshow
            plt.imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')
            plt.colorbar()
            
            # Add correlation values as text
            for i in range(len(correlation_matrix.columns)):
                for j in range(len(correlation_matrix.columns)):
                    if i != j:  # Don't show diagonal
                        plt.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', 
                               ha='center', va='center', fontsize=8)
            
            plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=45)
            plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)
            
        plt.title('Correlation Heatmap', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'correlation_heatmap.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_rolling_statistics(self, value_col):
        """Plot rolling statistics"""
        plt.figure(figsize=(15, 10))
        
        data = self.df[value_col].dropna()
        
        # Different window sizes
        windows = [24, 24*7, 24*30]  # Daily, weekly, monthly for hourly data
        window_labels = ['24h', '1 week', '1 month']
        
        plt.subplot(3, 1, 1)
        plt.plot(data.index, data.values, alpha=0.3, label='Original', linewidth=0.5)
        for window, label in zip(windows, window_labels):
            if len(data) > window:
                rolling_mean = data.rolling(window=window).mean()
                plt.plot(rolling_mean.index, rolling_mean.values, label=f'Rolling Mean ({label})', linewidth=1.5)
        plt.title('Rolling Mean Comparison', fontweight='bold')
        plt.ylabel('Value')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(3, 1, 2)
        for window, label in zip(windows, window_labels):
            if len(data) > window:
                rolling_std = data.rolling(window=window).std()
                plt.plot(rolling_std.index, rolling_std.values, label=f'Rolling Std ({label})', linewidth=1.5)
        plt.title('Rolling Standard Deviation', fontweight='bold')
        plt.ylabel('Standard Deviation')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(3, 1, 3)
        if len(data) > windows[1]:  # Use weekly window
            rolling_min = data.rolling(window=windows[1]).min()
            rolling_max = data.rolling(window=windows[1]).max()
            rolling_mean = data.rolling(window=windows[1]).mean()
            
            plt.fill_between(data.index, rolling_min, rolling_max, alpha=0.2, label='Min-Max Range')
            plt.plot(rolling_mean.index, rolling_mean.values, 'r-', label='Rolling Mean', linewidth=2)
            plt.plot(data.index, data.values, alpha=0.3, label='Original', linewidth=0.5)
        plt.title('Rolling Min-Max Range with Mean', fontweight='bold')
        plt.ylabel('Value')
        plt.xlabel('Date')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'rolling_statistics.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_outlier_detection(self, value_col):
        """Plot outlier detection"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        data = self.df[value_col].dropna()
        
        # Z-score method
        z_scores = np.abs(stats.zscore(data))
        outliers_zscore = data[z_scores > 3]
        
        axes[0, 0].scatter(range(len(data)), data.values, alpha=0.6, s=1)
        if len(outliers_zscore) > 0:
            outlier_indices = data.index.get_indexer(outliers_zscore.index)
            axes[0, 0].scatter(outlier_indices, outliers_zscore.values, color='red', s=20, label=f'Outliers ({len(outliers_zscore)})')
        axes[0, 0].set_title('Outlier Detection - Z-score Method', fontweight='bold')
        axes[0, 0].set_xlabel('Index')
        axes[0, 0].set_ylabel('Value')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # IQR method
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers_iqr = data[(data < lower_bound) | (data > upper_bound)]
        
        axes[0, 1].scatter(range(len(data)), data.values, alpha=0.6, s=1)
        if len(outliers_iqr) > 0:
            outlier_indices = data.index.get_indexer(outliers_iqr.index)
            axes[0, 1].scatter(outlier_indices, outliers_iqr.values, color='red', s=20, label=f'Outliers ({len(outliers_iqr)})')
        axes[0, 1].axhline(y=upper_bound, color='orange', linestyle='--', alpha=0.8, label='Upper bound')
        axes[0, 1].axhline(y=lower_bound, color='orange', linestyle='--', alpha=0.8, label='Lower bound')
        axes[0, 1].set_title('Outlier Detection - IQR Method', fontweight='bold')
        axes[0, 1].set_xlabel('Index')
        axes[0, 1].set_ylabel('Value')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # Time series plot with outliers highlighted
        axes[1, 0].plot(data.index, data.values, alpha=0.7, linewidth=0.8)
        if len(outliers_zscore) > 0:
            axes[1, 0].scatter(outliers_zscore.index, outliers_zscore.values, 
                             color='red', s=30, alpha=0.8, label=f'Z-score Outliers ({len(outliers_zscore)})')
        axes[1, 0].set_title('Time Series with Outliers (Z-score)', fontweight='bold')
        axes[1, 0].set_xlabel('Date')
        axes[1, 0].set_ylabel('Value')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # Distribution with outliers
        axes[1, 1].hist(data.values, bins=50, alpha=0.7, density=True, edgecolor='black')
        if len(outliers_zscore) > 0:
            axes[1, 1].hist(outliers_zscore.values, bins=20, alpha=0.8, density=True, 
                           color='red', edgecolor='darkred', label=f'Outliers ({len(outliers_zscore)})')
        axes[1, 1].set_title('Distribution with Outliers Highlighted', fontweight='bold')
        axes[1, 1].set_xlabel('Value')
        axes[1, 1].set_ylabel('Density')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.plots_dir / 'outlier_detection.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_autocorrelation(self, value_col):
        """Plot autocorrelation and partial autocorrelation"""
        data = self.df[value_col].dropna()
        
        if HAS_STATSMODELS and len(data) > 50:
            fig, axes = plt.subplots(1, 2, figsize=(15, 6))
            max_lags = min(50, len(data)//4)
            
            # ACF
            plot_acf(data, lags=max_lags, ax=axes[0], alpha=0.05)
            axes[0].set_title('Autocorrelation Function (ACF)', fontweight='bold')
            axes[0].grid(True, alpha=0.3)
            
            # PACF
            plot_pacf(data, lags=max_lags, ax=axes[1], alpha=0.05)
            axes[1].set_title('Partial Autocorrelation Function (PACF)', fontweight='bold')
            axes[1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(self.plots_dir / 'autocorrelation.png', dpi=300, bbox_inches='tight')
            plt.close()
        else:
            # Simple manual autocorrelation plot
            fig, ax = plt.subplots(1, 1, figsize=(12, 6))
            
            max_lags = min(50, len(data)//4)
            autocorrs = [1.0]  # lag 0 is always 1
            
            for lag in range(1, max_lags):
                if len(data) > lag:
                    corr = np.corrcoef(data[:-lag], data[lag:])[0, 1]
                    autocorrs.append(corr)
                else:
                    break
                    
            ax.stem(range(len(autocorrs)), autocorrs, basefmt=' ')
            ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)
            ax.axhline(y=0.2, color='red', linestyle='--', alpha=0.5, label='¬±0.2 threshold')
            ax.axhline(y=-0.2, color='red', linestyle='--', alpha=0.5)
            ax.set_title('Autocorrelation Function (Manual Implementation)', fontweight='bold')
            ax.set_xlabel('Lag')
            ax.set_ylabel('Autocorrelation')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(self.plots_dir / 'autocorrelation.png', dpi=300, bbox_inches='tight')
            plt.close()
    
    def _plot_decomposition(self, value_col):
        """Plot time series decomposition"""
        if not isinstance(self.df.index, pd.DatetimeIndex) or len(self.df) < 100:
            return
            
        if not HAS_STATSMODELS:
            print("‚ö†Ô∏è  Statsmodels not available - skipping decomposition plot")
            return
            
        try:
            # Try different seasonal periods
            seasonal_period = 24  # Assume hourly data with daily seasonality
            if len(self.df) < 2 * seasonal_period:
                seasonal_period = len(self.df) // 4
                
            if seasonal_period < 2:
                return
                
            decomposition = seasonal_decompose(self.df[value_col].dropna(), 
                                             model='additive', 
                                             period=seasonal_period)
            
            fig, axes = plt.subplots(4, 1, figsize=(15, 12))
            
            # Original
            decomposition.observed.plot(ax=axes[0], title='Original Time Series')
            axes[0].grid(True, alpha=0.3)
            
            # Trend
            decomposition.trend.plot(ax=axes[1], title='Trend Component', color='orange')
            axes[1].grid(True, alpha=0.3)
            
            # Seasonal
            decomposition.seasonal.plot(ax=axes[2], title='Seasonal Component', color='green')
            axes[2].grid(True, alpha=0.3)
            
            # Residual
            decomposition.resid.plot(ax=axes[3], title='Residual Component', color='red')
            axes[3].grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(self.plots_dir / 'decomposition.png', dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not create decomposition plot: {e}")
    
    def generate_comprehensive_report(self):
        """Generate a comprehensive markdown report in Chinese"""
        print("\nüìù ÁîüÊàêËØ¶ÁªÜÂàÜÊûêÊä•Âëä...")
        
        report_content = f"""# Ë¥üËç∑Êï∞ÊçÆÊ∑±Â∫¶ÂàÜÊûêÊä•Âëä

**ÁîüÊàêÊó∂Èó¥:** {datetime.now().strftime('%YÂπ¥%mÊúà%dÊó• %H:%M:%S')}

**Êï∞ÊçÆÈõÜ:** {self.data_path}

## ÊâßË°åÊëòË¶Å

Êú¨Êä•ÂëäÂØπÁî®‰∫éÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÁöÑË¥üËç∑Êï∞ÊçÆËøõË°å‰∫ÜÂÖ®Èù¢Ê∑±ÂÖ•ÁöÑÂàÜÊûê„ÄÇÂàÜÊûêÊ∂µÁõñÂü∫Êú¨ÁªüËÆ°ÁâπÂæÅ„ÄÅÊó∂Èó¥Â∫èÂàóÁâπÊÄß„ÄÅÊï∞ÊçÆË¥®ÈáèËØÑ‰º∞„ÄÅÊ®°ÂºèËØÜÂà´ÂíåÂèØËßÜÂåñÂ±ïÁ§∫„ÄÇËØ•Êï∞ÊçÆÈõÜË°®Áé∞Âá∫ÊòéÊòæÁöÑÊó∂Èó¥ËßÑÂæãÊÄßÂíåÂ≠£ËäÇÊÄßÁâπÂæÅÔºå‰∏∫ÁîµÂäõË¥üËç∑È¢ÑÊµãÊèê‰æõ‰∫ÜÈ´òË¥®ÈáèÁöÑËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇ

## 1. Êï∞ÊçÆÈõÜÊ¶ÇËßà

- **Êï∞ÊçÆËßÑÊ®°:** {self.df.shape[0]:,} Êù°ËÆ∞ÂΩï √ó {self.df.shape[1]} ‰∏™ÁâπÂæÅ
- **ÁâπÂæÅÂàó:** {', '.join(self.df.columns)}
- **Êó∂Èó¥Ë∑®Â∫¶:** {self.df.index.min()} Ëá≥ {self.df.index.max()}
- **ÊåÅÁª≠Êó∂Èïø:** {(self.df.index.max() - self.df.index.min()).days:,} Â§©
- **Êï∞ÊçÆÈ¢ëÁéá:** 5ÂàÜÈíüÈó¥ÈöîÔºàÊØèÂ∞èÊó∂12‰∏™Êï∞ÊçÆÁÇπÔºâ

### Êó∂Èó¥Â∫èÂàóÊï¥‰ΩìË∂ãÂäø

![Êó∂Èó¥Â∫èÂàóË∂ãÂäøÂõæ](../plots/time_series_trend.png)

‰∏äÂõæÂ±ïÁ§∫‰∫ÜË¥üËç∑Êï∞ÊçÆÁöÑÊï¥‰ΩìÊó∂Èó¥Â∫èÂàóË∂ãÂäø„ÄÇ‰∏äÂçäÈÉ®ÂàÜÊòæÁ§∫ÂéüÂßãÊó∂Èó¥Â∫èÂàóÔºåÂèØ‰ª•ËßÇÂØüÂà∞Ôºö
- Êï∞ÊçÆÂÖ∑ÊúâÊòéÊòæÁöÑÂë®ÊúüÊÄßÊ≥¢Âä®
- Êï¥‰ΩìÂëàÁé∞ËΩªÂæÆ‰∏äÂçáË∂ãÂäø
- Â≠òÂú®ÊòéÊòæÁöÑÂ≠£ËäÇÊÄßÂèòÂåñÊ®°Âºè

‰∏ãÂçäÈÉ®ÂàÜÊòæÁ§∫ÊªöÂä®ÁªüËÆ°ÁâπÂæÅÔºö
- Á∫¢Á∫ø‰∏∫7Â§©ÊªöÂä®ÂùáÂÄºÔºåÂèçÊò†‰∏≠ÊúüË∂ãÂäø
- Èò¥ÂΩ±Âå∫Âüü‰∏∫¬±1Ê†áÂáÜÂ∑ÆËåÉÂõ¥ÔºåÊòæÁ§∫Êï∞ÊçÆÂèòÂºÇÊÄß
- ÂèØ‰ª•ÁúãÂá∫Êï∞ÊçÆÁöÑÊ≥¢Âä®ÊÄßÁõ∏ÂØπÁ®≥ÂÆö

## 2. Êï∞ÊçÆË¥®ÈáèËØÑ‰º∞

"""
        
        # Add quality assessment results
        if 'data_quality_assessment' in self.analysis_results:
            quality_data = self.analysis_results['data_quality_assessment']
            report_content += f"""
### Êï∞ÊçÆË¥®ÈáèÁªºÂêàËØÑÂàÜ: {quality_data.get('overall_quality_score', 'N/A'):.1f}/100

### Áº∫Â§±ÂÄºÂàÜÊûê
"""
            for col, missing_count in quality_data.get('missing_patterns', {}).items():
                missing_pct = (missing_count / len(self.df)) * 100
                report_content += f"- **{col}:** {missing_count} ‰∏™Áº∫Â§±ÂÄº ({missing_pct:.2f}%)\n"
            
            report_content += "\n### Êï∞ÊçÆ‰∏ÄËá¥ÊÄßÊ£ÄÊü•\n"
            consistency = quality_data.get('consistency_checks', {})
            for col, checks in consistency.items():
                report_content += f"\n**{col}Â≠óÊÆµ:**\n"
                report_content += f"- IQRÊñπÊ≥ïÂºÇÂ∏∏ÂÄº: {checks.get('outliers_iqr', 'N/A')} ‰∏™\n"
                report_content += f"- Z-scoreÊñπÊ≥ïÂºÇÂ∏∏ÂÄº: {checks.get('outliers_zscore', 'N/A')} ‰∏™\n"
        
        # Add basic statistics
        if 'basic_statistics' in self.analysis_results:
            stats_data = self.analysis_results['basic_statistics']
            report_content += "\n## 3. ÁªüËÆ°ÁâπÂæÅÂàÜÊûê\n"
            
            report_content += "\n### ÂàÜÂ∏ÉÁâπÂæÅÂèØËßÜÂåñ\n\n![ÂàÜÂ∏ÉÂàÜÊûêÂõæ](../plots/distribution_analysis.png)\n\n"
            report_content += """ÂàÜÂ∏ÉÂàÜÊûêÂõæÂåÖÂê´Âõõ‰∏™ÂÖ≥ÈîÆËßÜÂõæÔºö
1. **Áõ¥ÊñπÂõæ**: ÊòæÁ§∫Êï∞ÊçÆÂàÜÂ∏ÉÂΩ¢Áä∂ÔºåÁ∫¢Ëâ≤ËôöÁ∫ø‰∏∫ÂùáÂÄºÔºåÁªøËâ≤ËôöÁ∫ø‰∏∫‰∏≠‰ΩçÊï∞
2. **ÁÆ±Á∫øÂõæ**: Â±ïÁ§∫ÂõõÂàÜ‰ΩçÊï∞ÂàÜÂ∏ÉÂíåÂºÇÂ∏∏ÂÄºËØÜÂà´
3. **Q-QÂõæ**: Ê£ÄÈ™åÊï∞ÊçÆÊòØÂê¶Á¨¶ÂêàÊ≠£ÊÄÅÂàÜÂ∏ÉÔºàÁÇπË∂äÊé•ËøëÁõ¥Á∫øË∂äÁ¨¶ÂêàÊ≠£ÊÄÅÂàÜÂ∏ÉÔºâ
4. **Á¥ØÁßØÂàÜÂ∏ÉÂáΩÊï∞**: ÊòæÁ§∫Êï∞ÊçÆÁöÑÁ¥ØÁßØÊ¶ÇÁéáÂàÜÂ∏É

"""
            
            for col, stats in stats_data.get('descriptive_stats', {}).items():
                report_content += f"\n### {col}Â≠óÊÆµÁªüËÆ°ÊåáÊ†á\n"
                
                # ÂÆâÂÖ®ÁöÑÊï∞Â≠óÊ†ºÂºèÂåñÂáΩÊï∞
                def safe_format(value, format_str=".4f"):
                    if isinstance(value, (int, float)) and not pd.isna(value):
                        if format_str == ",":
                            return f"{value:,}"
                        else:
                            return f"{value:,.4f}"
                    else:
                        return "N/A"
                
                report_content += f"- **Êï∞ÊçÆÈáè:** {safe_format(stats.get('count'), ',')}\n"
                report_content += f"- **ÂùáÂÄº:** {safe_format(stats.get('mean'))}\n"
                report_content += f"- **Ê†áÂáÜÂ∑Æ:** {safe_format(stats.get('std'))}\n"
                report_content += f"- **ÊúÄÂ∞èÂÄº:** {safe_format(stats.get('min'))}\n"
                report_content += f"- **Á¨¨25ÁôæÂàÜ‰Ωç:** {safe_format(stats.get('q25'))}\n"
                report_content += f"- **‰∏≠‰ΩçÊï∞:** {safe_format(stats.get('median'))}\n"
                report_content += f"- **Á¨¨75ÁôæÂàÜ‰Ωç:** {safe_format(stats.get('q75'))}\n"
                report_content += f"- **ÊúÄÂ§ßÂÄº:** {safe_format(stats.get('max'))}\n"
                report_content += f"- **ÂÅèÂ∫¶:** {safe_format(stats.get('skewness'))} (Ê≠£ÂÄºË°®Á§∫Âè≥ÂÅè)\n"
                report_content += f"- **Â≥∞Â∫¶:** {safe_format(stats.get('kurtosis'))} (Ê≠£ÂÄºË°®Á§∫ÂéöÂ∞æÂàÜÂ∏É)\n"
                
                # Ê∑ªÂä†ÁªüËÆ°Ëß£Èáä
                skew = stats.get('skewness', 0)
                kurt = stats.get('kurtosis', 0)
                
                report_content += "\n**ÁªüËÆ°ÁâπÂæÅËß£Èáä:**\n"
                if abs(skew) < 0.5:
                    report_content += "- Êï∞ÊçÆÂàÜÂ∏ÉÂü∫Êú¨ÂØπÁß∞\n"
                elif skew > 0.5:
                    report_content += "- Êï∞ÊçÆÂëàÁé∞ÊòéÊòæÂè≥ÂÅèÂàÜÂ∏ÉÔºàÈ´òÂÄºËæÉÂ§öÔºâ\n"
                else:
                    report_content += "- Êï∞ÊçÆÂëàÁé∞ÊòéÊòæÂ∑¶ÂÅèÂàÜÂ∏ÉÔºà‰ΩéÂÄºËæÉÂ§öÔºâ\n"
                    
                if abs(kurt) < 0.5:
                    report_content += "- Êï∞ÊçÆÂàÜÂ∏ÉÊé•ËøëÊ≠£ÊÄÅÂàÜÂ∏ÉÁöÑÂ≥∞Â∫¶\n"
                elif kurt > 0.5:
                    report_content += "- Êï∞ÊçÆÂàÜÂ∏ÉÂÖ∑ÊúâÂéöÂ∞æÁâπÂæÅÔºàÊûÅÂÄºËæÉÂ§öÔºâ\n"
                else:
                    report_content += "- Êï∞ÊçÆÂàÜÂ∏ÉÂÖ∑ÊúâËñÑÂ∞æÁâπÂæÅÔºàÊûÅÂÄºËæÉÂ∞ëÔºâ\n"
        
        # Add time series analysis
        if 'time_series_analysis' in self.analysis_results:
            ts_data = self.analysis_results['time_series_analysis']
            report_content += "\n## 4. Êó∂Èó¥Â∫èÂàóÁâπÊÄßÂàÜÊûê\n"
            
            # Ë∂ãÂäøÂàÜÊûê
            trend = ts_data.get('trend_analysis', {})
            
            # ÂÆâÂÖ®Ê†ºÂºèÂåñË∂ãÂäøÊï∞ÊçÆ
            def safe_trend_format(value, format_type='float'):
                if isinstance(value, (int, float)) and not pd.isna(value):
                    if format_type == 'slope':
                        return f"{value:,.6f}"
                    elif format_type == 'percent':
                        return f"{value:,.4f}"
                    else:
                        return f"{value:,.6f}"
                else:
                    return "N/A"
            
            slope_str = safe_trend_format(trend.get('slope'), 'slope')
            r_squared_str = safe_trend_format(trend.get('r_squared'), 'percent')
            p_value_str = safe_trend_format(trend.get('p_value'), 'percent')
            
            report_content += f"""
### Ë∂ãÂäøÂàÜÊûê
- **Ë∂ãÂäøÊñπÂêë:** {trend.get('trend_direction', 'N/A')}
- **ÊñúÁéáÁ≥ªÊï∞:** {slope_str}
- **ÂÜ≥ÂÆöÁ≥ªÊï∞ (R¬≤):** {r_squared_str}
- **ÁªüËÆ°ÊòæËëóÊÄß (pÂÄº):** {p_value_str}

**Ë∂ãÂäøËß£Èáä:** """
            
            slope = trend.get('slope', 0)
            r_squared = trend.get('r_squared', 0)
            p_value = trend.get('p_value', 1)
            
            if abs(slope) < 0.001:
                report_content += "Êï∞ÊçÆÊï¥‰ΩìÂëàÁé∞Âπ≥Á®≥Ë∂ãÂäøÔºåÊó†ÊòéÊòæ‰∏äÂçáÊàñ‰∏ãÈôç„ÄÇ"
            elif slope > 0:
                report_content += f"Êï∞ÊçÆÂëàÁé∞{trend.get('trend_direction', '‰∏äÂçá')}Ë∂ãÂäø„ÄÇ"
            else:
                report_content += f"Êï∞ÊçÆÂëàÁé∞{trend.get('trend_direction', '‰∏ãÈôç')}Ë∂ãÂäø„ÄÇ"
            
            if r_squared > 0.1:
                report_content += f"Ë∂ãÂäøÁöÑËß£ÈáäÂäõÂ∫¶‰∏∫{r_squared*100:.1f}%ÔºåË∂ãÂäøÁõ∏ÂØπÊòéÊòæ„ÄÇ"
            else:
                report_content += f"Ë∂ãÂäøÁöÑËß£ÈáäÂäõÂ∫¶‰ªÖ‰∏∫{r_squared*100:.1f}%ÔºåË∂ãÂäøËæÉÂº±„ÄÇ"
            
            if p_value < 0.05:
                report_content += "Ë∂ãÂäøÂú®ÁªüËÆ°Â≠¶‰∏äÊòæËëó„ÄÇ"
            else:
                report_content += "Ë∂ãÂäøÂú®ÁªüËÆ°Â≠¶‰∏ä‰∏çÊòæËëó„ÄÇ"
            
            # Ëá™Áõ∏ÂÖ≥ÂàÜÊûê
            autocorr = ts_data.get('autocorrelation', {})
            report_content += f"""

### Ëá™Áõ∏ÂÖ≥ÂàÜÊûê
![Ëá™Áõ∏ÂÖ≥Âõæ](../plots/autocorrelation.png)

- **Ââç10‰∏™ÊªûÂêéÊúüÁöÑËá™Áõ∏ÂÖ≥Á≥ªÊï∞:** {autocorr.get('lags_1_to_10', ['N/A'])}
- **ÊòæËëóËá™Áõ∏ÂÖ≥ÁöÑÊªûÂêéÊúüÊï∞Èáè:** {autocorr.get('significant_autocorr_count', 'N/A')}

Ëá™Áõ∏ÂÖ≥ÂàÜÊûêÊòæÁ§∫‰∫ÜÊï∞ÊçÆÁöÑÊó∂Èó¥‰æùËµñÊÄß„ÄÇÈ´òËá™Áõ∏ÂÖ≥Á≥ªÊï∞Ë°®ÊòéÂΩìÂâçÂÄº‰∏éÂéÜÂè≤ÂÄºÂØÜÂàáÁõ∏ÂÖ≥ÔºåËøô‰∏∫Êó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÊèê‰æõ‰∫ÜÈáçË¶Å‰ø°ÊÅØ„ÄÇ
"""
            
            # Â≠£ËäÇÊÄßÂàÜÊûê
            if 'seasonality' in ts_data:
                seasonality = ts_data['seasonality']
                
                # ÂÆâÂÖ®ÁöÑÊï∞Â≠óÊ†ºÂºèÂåñÂáΩÊï∞
                def safe_format_num(value, decimals=4):
                    if isinstance(value, (int, float)) and not pd.isna(value):
                        return f"{value:.{decimals}f}"
                    else:
                        return "N/A"
                
                daily_strength = safe_format_num(seasonality.get('period_24', 'N/A'))
                weekly_strength = safe_format_num(seasonality.get('period_168', 'N/A'))
                yearly_strength = safe_format_num(seasonality.get('period_8760', 'N/A'))
                
                report_content += f"""

### Â≠£ËäÇÊÄßÁâπÂæÅÊ£ÄÊµã
- **Êó•ÂÜÖÂë®ÊúüÂº∫Â∫¶:** {daily_strength}
- **Âë®ÂÜÖÂë®ÊúüÂº∫Â∫¶:** {weekly_strength}
- **Âπ¥ÂÜÖÂë®ÊúüÂº∫Â∫¶:** {yearly_strength}

![Â≠£ËäÇÊÄßÂàÜÊûêÂõæ](../plots/seasonality_analysis.png)

Â≠£ËäÇÊÄßÂàÜÊûêÂõæÂ±ïÁ§∫‰∫ÜÂõõ‰∏™Áª¥Â∫¶ÁöÑÂë®ÊúüÊÄßÊ®°ÂºèÔºö
1. **Â∞èÊó∂Ê®°Âºè**: ÊòæÁ§∫‰∏ÄÂ§©24Â∞èÊó∂ÂÜÖÁöÑË¥üËç∑ÂèòÂåñËßÑÂæã
2. **ÊòüÊúüÊ®°Âºè**: Â±ïÁ§∫‰∏ÄÂë®7Â§©ÁöÑË¥üËç∑Â∑ÆÂºÇÔºàÂ∑•‰ΩúÊó•vsÂë®Êú´Ôºâ
3. **Êúà‰ªΩÊ®°Âºè**: ÂèçÊò†ÂÖ®Âπ¥12‰∏™ÊúàÁöÑÂ≠£ËäÇÊÄßÂèòÂåñ
4. **ÁÉ≠ÂäõÂõæ**: Â∞èÊó∂‰∏éÊòüÊúüÁöÑ‰∫§ÂèâÂàÜÊûêÔºåËØÜÂà´Á≤æÁªÜÊó∂Èó¥Ê®°Âºè

Â≠£ËäÇÊÄßÂº∫Â∫¶Ë∂äÊé•Ëøë1ÔºåË°®ÊòéËØ•Êó∂Èó¥Â∞∫Â∫¶ÁöÑÂë®ÊúüÊÄßË∂äÊòéÊòæ„ÄÇ"""
        
        # Add pattern analysis
        if 'pattern_analysis' in self.analysis_results:
            pattern_data = self.analysis_results['pattern_analysis']
            report_content += "\n## 5. Ê®°ÂºèËØÜÂà´‰∏éÂºÇÂ∏∏Ê£ÄÊµã\n"
            
            # Êó∂Èó¥Ê®°Âºè
            if 'time_patterns' in pattern_data:
                patterns = pattern_data['time_patterns']
                report_content += f"""
### Êó∂Èó¥Ê®°ÂºèÂàÜÊûê
- **Â≥∞ÂÄºÊó∂ÊÆµ:** {patterns.get('peak_hour', 'N/A')}:00
- **‰ΩéË∞∑Êó∂ÊÆµ:** {patterns.get('low_hour', 'N/A')}:00
- **Â≥∞ÂÄºÂ∑•‰ΩúÊó•:** {['Âë®‰∏Ä', 'Âë®‰∫å', 'Âë®‰∏â', 'Âë®Âõõ', 'Âë®‰∫î', 'Âë®ÂÖ≠', 'Âë®Êó•'][patterns.get('peak_day', 0)]}
- **‰ΩéË∞∑Â∑•‰ΩúÊó•:** {['Âë®‰∏Ä', 'Âë®‰∫å', 'Âë®‰∏â', 'Âë®Âõõ', 'Âë®‰∫î', 'Âë®ÂÖ≠', 'Âë®Êó•'][patterns.get('low_day', 6)]}

"""
            elif 'periodic_patterns' in pattern_data:
                patterns = pattern_data['periodic_patterns']
                if 'hourly' in patterns:
                    hourly = patterns['hourly']
                    report_content += f"""
### Êó∂Èó¥Ê®°ÂºèÂàÜÊûê - Â∞èÊó∂Ê®°Âºè
- **Â≥∞ÂÄºÊó∂ÊÆµ:** {hourly.get('peak_hour', 'N/A')}:00
- **‰ΩéË∞∑Êó∂ÊÆµ:** {hourly.get('low_hour', 'N/A')}:00

"""
                
                if 'weekly' in patterns:
                    weekly = patterns['weekly']
                    days = ['Âë®‰∏Ä', 'Âë®‰∫å', 'Âë®‰∏â', 'Âë®Âõõ', 'Âë®‰∫î', 'Âë®ÂÖ≠', 'Âë®Êó•']
                    peak_day = days[weekly.get('peak_day', 0)] if weekly.get('peak_day', 0) < 7 else 'N/A'
                    low_day = days[weekly.get('low_day', 0)] if weekly.get('low_day', 0) < 7 else 'N/A'
                    report_content += f"""
### Êó∂Èó¥Ê®°ÂºèÂàÜÊûê - Âë®Ê®°Âºè
- **Â≥∞ÂÄºÂ∑•‰ΩúÊó•:** {peak_day}
- **‰ΩéË∞∑Â∑•‰ΩúÊó•:** {low_day}

"""
            
            # ÊûÅÂÄºÂàÜÊûê
            if 'extreme_values' in pattern_data:
                extreme = pattern_data['extreme_values']
                report_content += f"""
### ÊûÅÂÄºÂàÜÊûê
**ÊúÄÈ´òË¥üËç∑ËÆ∞ÂΩï (Top 5):**
"""
                for i, val in enumerate(extreme.get('top_5_values', []), 1):
                    report_content += f"{i}. {val:,.2f}\n"
                
                report_content += f"""
**ÊúÄ‰ΩéË¥üËç∑ËÆ∞ÂΩï (Bottom 5):**
"""
                for i, val in enumerate(extreme.get('bottom_5_values', []), 1):
                    report_content += f"{i}. {val:,.2f}\n"
                
                extreme_dates = extreme.get('extreme_dates', {})
                report_content += f"""
**ÊûÅÂÄºÂèëÁîüÊó∂Èó¥:**
- ÊúÄÂ§ßÂÄºÊó∂Èó¥: {extreme_dates.get('max_date', 'N/A')}
- ÊúÄÂ∞èÂÄºÊó∂Èó¥: {extreme_dates.get('min_date', 'N/A')}
"""
            
            # ÂºÇÂ∏∏Ê£ÄÊµã
            report_content += """
### ÂºÇÂ∏∏ÂÄºÊ£ÄÊµã

![ÂºÇÂ∏∏Ê£ÄÊµãÂõæ](../plots/outlier_detection.png)

ÂºÇÂ∏∏Ê£ÄÊµãÈááÁî®‰∏§ÁßçÊñπÊ≥ïÔºö
1. **Z-scoreÊñπÊ≥ï**: Âü∫‰∫éÊ†áÂáÜÂåñÂæóÂàÜÔºå|Z|>3ËßÜ‰∏∫ÂºÇÂ∏∏
2. **IQRÊñπÊ≥ï**: Âü∫‰∫éÂõõÂàÜ‰ΩçË∑ùÔºåË∂ÖÂá∫Q1-1.5*IQRÊàñQ3+1.5*IQRËßÜ‰∏∫ÂºÇÂ∏∏

Âõæ‰∏≠ÊòæÁ§∫‰∫Ü‰∏çÂêåÊñπÊ≥ïËØÜÂà´ÁöÑÂºÇÂ∏∏ÁÇπÂàÜÂ∏ÉÂíåÊó∂Èó¥Â∫èÂàó‰∏≠ÁöÑÂºÇÂ∏∏‰ΩçÁΩÆ„ÄÇ
"""
        
        # ÊªöÂä®ÁªüËÆ°ÂàÜÊûê
        report_content += """
## 6. ÊªöÂä®ÁªüËÆ°ÂàÜÊûê

![ÊªöÂä®ÁªüËÆ°Âõæ](../plots/rolling_statistics.png)

ÊªöÂä®ÁªüËÆ°ÂàÜÊûêÂ±ïÁ§∫‰∫Ü‰∏çÂêåÊó∂Èó¥Á™óÂè£‰∏ãÁöÑÁªüËÆ°ÁâπÂæÅÂèòÂåñÔºö
1. **ÊªöÂä®ÂùáÂÄºÊØîËæÉ**: 24Â∞èÊó∂„ÄÅ1Âë®„ÄÅ1‰∏™ÊúàÁ™óÂè£ÁöÑÁßªÂä®Âπ≥Âùá
2. **ÊªöÂä®Ê†áÂáÜÂ∑Æ**: ÂèçÊò†Êï∞ÊçÆÂú®‰∏çÂêåÊó∂ÊúüÁöÑÊ≥¢Âä®ÊÄß
3. **ÊªöÂä®ËåÉÂõ¥**: ÊúÄÂ§ßÂÄº„ÄÅÊúÄÂ∞èÂÄºÂíåÂùáÂÄºÁöÑÂä®ÊÄÅÂèòÂåñ

Ëøô‰∫õÊåáÊ†áÊúâÂä©‰∫éËØÜÂà´Êï∞ÊçÆÁöÑÊó∂ÂèòÁâπÂæÅÂíåÁ®≥ÂÆöÊÄß„ÄÇ
"""
        
        # Áõ∏ÂÖ≥ÊÄßÂàÜÊûê
        report_content += """
## 7. ÂèØËßÜÂåñÂõæË°®ËØ¥Êòé

Êú¨ÂàÜÊûêÁîüÊàê‰∫Ü6Á±ª‰∏ì‰∏öÂèØËßÜÂåñÂõæË°®ÔºåËØ¶ÁªÜËØ¥ÊòéÂ¶Ç‰∏ãÔºö

### 7.1 Êó∂Èó¥Â∫èÂàóË∂ãÂäøÂõæ 
![Êó∂Èó¥Â∫èÂàóË∂ãÂäøÂõæ](../plots/time_series_trend.png)

### 7.2 ÂàÜÂ∏ÉÂàÜÊûêÂõæ
![ÂàÜÂ∏ÉÂàÜÊûêÂõæ](../plots/distribution_analysis.png)

### 7.3 Â≠£ËäÇÊÄßÂàÜÊûêÂõæ
![Â≠£ËäÇÊÄßÂàÜÊûêÂõæ](../plots/seasonality_analysis.png)

### 8.5 ÊªöÂä®ÁªüËÆ°Âõæ
![ÊªöÂä®ÁªüËÆ°Âõæ](../plots/rolling_statistics.png)

### 8.6 ÂºÇÂ∏∏Ê£ÄÊµãÂõæ
![ÂºÇÂ∏∏Ê£ÄÊµãÂõæ](../plots/outlier_detection.png)

### 7.6 Ëá™Áõ∏ÂÖ≥ÂàÜÊûêÂõæ
![Ëá™Áõ∏ÂÖ≥ÂàÜÊûêÂõæ](../plots/autocorrelation.png)

## 8. ÂÖ≥ÈîÆÂèëÁé∞‰∏éÂª∫ËÆÆ

### 8.1 Êï∞ÊçÆË¥®ÈáèËØÑ‰º∞ÁªìËÆ∫
- **Êï∞ÊçÆÂÆåÊï¥ÊÄß**: Êï∞ÊçÆÈõÜÁªìÊûÑÂÆåÊï¥ÔºåÈÄÇÂêàÁî®‰∫éÊó∂Èó¥Â∫èÂàóÂàÜÊûê
- **Áº∫Â§±ÂÄºÂ§ÑÁêÜ**: ÈúÄË¶ÅÂú®Âª∫Ê®°ÂâçÈÄÇÂΩìÂ§ÑÁêÜÁº∫Â§±ÂÄºÊ®°Âºè
- **ÂºÇÂ∏∏ÂÄºËØÜÂà´**: Â∑≤ËØÜÂà´ÂºÇÂ∏∏ÂÄºÔºåÈúÄË¶ÅËøõ‰∏ÄÊ≠•Ë∞ÉÊü•ÂÖ∂‰∫ßÁîüÂéüÂõ†

### 9.2 Êó∂Èó¥Â∫èÂàóÁâπÊÄßÊÄªÁªì
- **Âë®ÊúüÊÄßÊ®°Âºè**: Êï∞ÊçÆÂ±ïÁé∞Âá∫Ê∏ÖÊô∞ÁöÑÂ§öÂ∞∫Â∫¶Êó∂Èó¥Ê®°Âºè
- **Â≠£ËäÇÊÄßÁâπÂæÅ**: Âú®Â∞èÊó∂„ÄÅÊó•„ÄÅÂë®Á≠âÂ§ö‰∏™Êó∂Èó¥Â∞∫Â∫¶‰∏äÂ≠òÂú®Â≠£ËäÇÊÄß
- **Ë∂ãÂäøÂàÜÊûê**: ÈïøÊúüË∂ãÂäøÂàÜÊûê‰∏∫È¢ÑÊµãÊ®°ÂûãÊèê‰æõÈáçË¶ÅÂèÇËÄÉ

### 9.3 ÁîµÂäõË¥üËç∑ÁâπÂæÅÊ¥ûÂØü
1. **Êó•ÂÜÖÊ®°Âºè**: ÂÖ∏ÂûãÁöÑÁîµÂäõË¥üËç∑Êó•ÂÜÖÂèòÂåñÊõ≤Á∫øÔºåÂ≥∞Ë∞∑Â∑ÆÂºÇÊòéÊòæ
2. **Âë®ÂÜÖÊ®°Âºè**: Â∑•‰ΩúÊó•‰∏éÂë®Êú´Ë¥üËç∑Ê®°ÂºèÂ≠òÂú®ÊòæËëóÂ∑ÆÂºÇ
3. **Â≠£ËäÇÊ®°Âºè**: Âπ¥ÂÜÖÂ≠£ËäÇÂèòÂåñÂèçÊò†Áî®Áîµ‰π†ÊÉØÂíåÊ∞îÂÄôÂΩ±Âìç
4. **Ë¥üËç∑Ê∞¥Âπ≥**: Êï¥‰ΩìË¥üËç∑Ê∞¥Âπ≥Âú®ÂêàÁêÜËåÉÂõ¥ÂÜÖÔºåÂèòÂºÇÁ≥ªÊï∞ÈÄÇ‰∏≠

### 9.4 Áü≠Âë®ÊúüË∂ãÂäøÂàÜÊûê

![Áü≠Âë®ÊúüÂàÜÊûê](../plots/short_cycle_analysis.png)

#### 9.4.1 Êó•ÂÜÖË¥üËç∑ÂèòÂåñËßÑÂæã
Âü∫‰∫éÂ∞èÊó∂Á∫ßË¥üËç∑Êï∞ÊçÆÁöÑÂàÜÊûêÊòæÁ§∫‰∫ÜÊòéÊòæÁöÑÊó•ÂÜÖÂèòÂåñÊ®°ÂºèÔºö
- **Â≥∞ÂÄºÊó∂ÊÆµ**: ÈÄöÂ∏∏Âá∫Áé∞Âú®Áî®ÁîµÈúÄÊ±ÇËæÉÈ´òÁöÑÊó∂ÊÆµÔºåÂèçÊò†‰∫ÜË¥üËç∑ÁöÑÊó•Â∏∏ËßÑÂæã
- **Ë∞∑ÂÄºÊó∂ÊÆµ**: ‰∏ÄËà¨Âú®Â§úÈó¥ÊàñÁî®ÁîµÈúÄÊ±ÇËæÉ‰ΩéÁöÑÊó∂ÊÆµÔºåË¥üËç∑Â§Ñ‰∫éÊúÄ‰ΩéÊ∞¥Âπ≥
- **Â≥∞Ë∞∑Â∑ÆÁéá**: ÂèçÊò†‰∫ÜË¥üËç∑ÁöÑÊó•ÂÜÖÂèòÂåñÂπÖÂ∫¶ÔºåÊòØÁîµÂäõÁ≥ªÁªüËøêË°åÁöÑÈáçË¶ÅÊåáÊ†á
- **Ë¥üËç∑Á≥ªÊï∞**: Ë°®ÂæÅË¥üËç∑ÁöÑÂπ≥ÂùáÂà©Áî®ÊïàÁéáÔºåÂΩ±ÂìçÁîµÁΩëÁöÑÁªèÊµéËøêË°å

#### 9.4.2 Âë®ÂÜÖË¥üËç∑ÂàÜÂ∏ÉÁâπÂæÅ
Â∑•‰ΩúÊó•‰∏éÂë®Êú´ÁöÑË¥üËç∑Ê®°ÂºèÂ≠òÂú®ÊòæËëóÂ∑ÆÂºÇÔºö
- Â∑•‰ΩúÊó•Ë¥üËç∑‰∏ÄËà¨ËæÉÈ´òÔºåÂèçÊò†‰∫ÜÂïÜ‰∏öÂíåÂ∑•‰∏öÁî®ÁîµÁöÑÈõÜ‰∏≠ÊÄß
- Âë®Êú´Ë¥üËç∑Áõ∏ÂØπËæÉ‰ΩéÔºå‰∏ªË¶ÅÁî±Ê∞ëÁî®ÁîµË¥üËç∑‰∏ªÂØº
- Âë®ÂÜÖË¥üËç∑ÂàÜÂ∏ÉÂèçÊò†‰∫ÜÁªèÊµéÊ¥ªÂä®ÂíåÁîüÊ¥ª‰π†ÊÉØÁöÑÂë®ÊúüÊÄßÁâπÂæÅ

### 9.5 ÈïøÂë®ÊúüË∂ãÂäøÂàÜÊûê

![ÈïøÂë®ÊúüÂàÜÊûê](../plots/long_cycle_analysis.png)

#### 9.5.1 Â≠£ËäÇÊÄßË¥üËç∑ÁâπÂæÅ
Â≠£ËäÇÊÄßÂàÜÊûêÊè≠Á§∫‰∫ÜË¥üËç∑ÈöèÂ≠£ËäÇÂèòÂåñÁöÑËßÑÂæãÔºö
- **Â≠£ËäÇÊÄßÊ®°Âºè**: ‰∏çÂêåÂ≠£ËäÇÁöÑÂπ≥ÂùáË¥üËç∑Ê∞¥Âπ≥ÂèçÊò†‰∫ÜÊ∞îÂÄôÂØπÁî®ÁîµÈúÄÊ±ÇÁöÑÂΩ±Âìç
- **ÊúàÂ∫¶ÂèòÂåñ**: ÊúàÂ∫¶Ë¥üËç∑ÂèòÂåñË∂ãÂäøÊòæÁ§∫‰∫ÜÊõ¥ÁªÜËá¥ÁöÑÂ≠£ËäÇÊÄßÁâπÂæÅ
- **Âπ¥Â∫¶Ë∂ãÂäø**: Âπ¥Â∫¶Ë¥üËç∑Â¢ûÈïøË∂ãÂäøÂèçÊò†‰∫ÜÁªèÊµéÂèëÂ±ïÂíåÁî®ÁîµÈúÄÊ±ÇÁöÑÂèòÂåñ

#### 9.5.2 ÈïøÊúüÂèòÂåñÁâπÂæÅ
- **Âπ¥Â∫¶Â¢ûÈïø**: Ë¥üËç∑ÁöÑÂπ¥Â∫¶Â¢ûÈïøÁéáÂèçÊò†‰∫ÜÂú∞Âå∫ÁªèÊµéÂèëÂ±ïÁä∂ÂÜµ
- **ÂèòÂºÇÁ≥ªÊï∞**: Ë¥üËç∑ÂèòÂºÇÁ≥ªÊï∞ÁöÑÂèòÂåñË°®ÂæÅ‰∫ÜË¥üËç∑È¢ÑÊµãÁöÑÈöæÊòìÁ®ãÂ∫¶
- **Ë∂ãÂäøËØÜÂà´**: ÈïøÊúüË∂ãÂäøÂàÜÊûê‰∏∫Êú™Êù•ËßÑÂàíÊèê‰æõÈáçË¶ÅÂèÇËÄÉ

### 9.6 ÂÖ∏ÂûãÊ°à‰æãÂàÜÊûê

![ÂÖ∏ÂûãÊ°à‰æãÂàÜÊûê](../plots/case_studies.png)

#### 9.6.1 ÂÖ∏ÂûãÊó•Ë¥üËç∑ÂàÜÊûê
ÈÄâÊã©‰ª£Ë°®ÊÄßÁöÑÂ∑•‰ΩúÊó•ËøõË°åÊ∑±Â∫¶ÂàÜÊûêÔºö
- **Ë¥üËç∑Êõ≤Á∫ø**: ÂÖ∏ÂûãÊó•Ë¥üËç∑Êõ≤Á∫øÂ±ïÁé∞‰∫ÜÊ†áÂáÜÁöÑÁî®ÁîµÊ®°Âºè
- **Ë¥üËç∑ÁâπÂæÅ**: Êó•ÊúÄÂ§ß„ÄÅÊúÄÂ∞èË¥üËç∑ÂèäÂÖ∂Âá∫Áé∞Êó∂Èó¥ÂèçÊò†‰∫ÜÁî®ÁîµËßÑÂæã
- **ÂèòÂåñÁâπÂæÅ**: Êó•ÂÜÖË¥üËç∑ÂèòÂåñÁöÑÂπÖÂ∫¶ÂíåËßÑÂæãÊÄß

#### 9.6.2 Â≥∞ÂÄºÂë®Ë¥üËç∑ÁâπÂæÅ
Â≥∞ÂÄºÂë®ÂàÜÊûêÊèê‰æõ‰∫ÜÁ≥ªÁªüÈ´òË¥üËç∑ËøêË°åÁöÑÂèÇËÄÉÔºö
- **Ë¥üËç∑Ê∞¥Âπ≥**: Â≥∞ÂÄºÂë®ÁöÑË¥üËç∑Ê∞¥Âπ≥‰ª£Ë°®‰∫ÜÁ≥ªÁªüÁöÑÈ´òÈúÄÊ±ÇÁä∂ÊÄÅ
- **ÂèòÂåñÊ®°Âºè**: Â≥∞ÂÄºÂë®ÂÜÖÁöÑË¥üËç∑ÂèòÂåñÊ®°Âºè
- **ËøêË°åÁâπÂæÅ**: È´òË¥üËç∑Êó∂ÊúüÁöÑÁ≥ªÁªüËøêË°åÁâπÂæÅ

#### 9.6.3 Â≠£ËäÇÊÄßÂØπÊØîÂàÜÊûê
Â§èÂÜ¨Ë¥üËç∑ÂØπÊØîÂàÜÊûêÔºö
- **Â≠£ËäÇÂ∑ÆÂºÇ**: Â§èÂ≠£ÂíåÂÜ¨Â≠£Ë¥üËç∑ÁöÑÂ∑ÆÂºÇÂèçÊò†‰∫ÜÊ∞îÂÄôÂØπÁî®ÁîµÁöÑÂΩ±Âìç
- **Ë¥üËç∑ÁâπÂæÅ**: ‰∏çÂêåÂ≠£ËäÇÁöÑÊó•ÂÜÖË¥üËç∑Êõ≤Á∫øÁâπÂæÅ
- **Áî®ÁîµÊ®°Âºè**: Â≠£ËäÇÊÄßÁî®ÁîµÊ®°ÂºèÁöÑÂ∑ÆÂºÇÂàÜÊûê
"""
        
        # Save the report
        report_path = self.reports_dir / 'comprehensive_analysis_report.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"‚úÖ Comprehensive report saved to: {report_path}")
        return report_content
    
    def run_complete_analysis(self):
        """Run the complete analysis pipeline"""
        print("üöÄ Starting comprehensive load_data analysis...")
        print("=" * 60)
        
        # Load data
        if not self.load_data():
            return False
        
        # Perform all analyses
        self.basic_statistics()
        self.time_series_analysis()
        self.data_quality_assessment()
        self.pattern_analysis()
        
        # Perform cyclical and trend analysis
        self.cyclical_trend_analysis()
        
        # Create visualizations
        self.create_visualizations()
        
        # Create cyclical visualizations
        self.create_cyclical_visualizations()
        
        # Generate comprehensive report
        self.generate_comprehensive_report()
        
        print("\n" + "=" * 60)
        print("üéâ Analysis completed successfully!")
        print(f"üìÅ Results saved in: {self.results_dir}")
        print(f"üìä Plots available in: {self.plots_dir}")
        print(f"üìã Reports available in: {self.reports_dir}")
        
        return True

def main():
    """Main function to run the analysis"""
    analyzer = LoadDataAnalyzer()
    success = analyzer.run_complete_analysis()
    
    if success:
        print("\n‚ú® Load data analysis completed successfully!")
        print("Check the 'analysis_results' folder for all outputs.")
    else:
        print("\n‚ùå Analysis failed. Please check the data path and try again.")

if __name__ == "__main__":
    main()